{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41d998-4c61-4072-b074-39142846f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~8GB\n",
    "!wget https://dsci6015s25-midterm.s3.us-east-1.amazonaws.com/X_train-002.dat\n",
    "!wget https://dsci6015s25-midterm.s3.us-east-1.amazonaws.com/X_test.dat\n",
    "!wget https://dsci6015s25-midterm.s3.us-east-1.amazonaws.com/y_train.dat\n",
    "!wget https://dsci6015s25-midterm.s3.us-east-1.amazonaws.com/y_test.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34c5c4-c81b-4c72-a790-46a47e1ef09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp X_train-002.dat vMalConv/X_train.dat\n",
    "!cp X_test.dat vMalConv/X_test.dat\n",
    "!cp y_train.dat vMalConv/y_train.dat\n",
    "!cp y_test.dat vMalConv/y_test.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cfd294-a4dd-4fbe-8901-befedebd2597",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lief (from -r requirements.txt (line 1))\n",
      "  Downloading lief-0.13.2-cp310-cp310-manylinux_2_24_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tqdm>=4.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.16.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.2.3)\n",
      "Collecting lightgbm>=2.2.3 (from -r requirements.txt (line 5))\n",
      "  Downloading lightgbm-4.6.0.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Installing build dependencies ... \u001bdone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
      "Collecting setuptools==58.0.0 (from -r requirements.txt (line 7))\n",
      "  Downloading setuptools-58.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2025.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from lightgbm>=2.2.3->-r requirements.txt (line 5)) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.20.3->-r requirements.txt (line 6)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.20.3->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->-r requirements.txt (line 4)) (1.17.0)\n",
      "Downloading setuptools-58.0.0-py3-none-any.whl (816 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.3/816.3 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lief-0.13.2-cp310-cp310-manylinux_2_24_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m139.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel done\n",
      "\u001b[?25h  Created wheel for lightgbm: filename=lightgbm-4.6.0-py3-none-linux_x86_64.whl size=2737779 sha256=f43c928bdc27b189cdbdea558b0c6291a32a452cd8840305450fd817cb27d895\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/bb/db/6d/7814aed03437129dc284a055c084f201b765deb54b6908efab\n",
      "Successfully built lightgbm\n",
      "Installing collected packages: setuptools, lief, lightgbm\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.8.0\n",
      "    Uninstalling setuptools-75.8.0:\n",
      "      Successfully uninstalled setuptools-75.8.0\n",
      "Successfully installed lief-0.13.2 lightgbm-4.6.0 setuptools-58.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05b658e-a4c7-421d-b68d-ae1d587b5f54",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PFGimenez/ember.git\n",
      "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-j_hsd__u\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-j_hsd__u\n",
      "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/PFGimenez/ember.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49be43e-bbde-4d3b-8e22-6d511fbc425f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ember\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22fbc65-c67f-40df-bed2-d5e21bec2414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
      "WARNING:   lief version 0.13.2-2d9855fc found instead. There may be slight inconsistencies\n",
      "WARNING:   in the feature calculations.\n",
      "X_train shape: (800000, 2381)\n",
      "y_train shape: (800000,)\n",
      "X_test shape: (200000, 2381)\n",
      "y_test shape: (200000,)\n"
     ]
    }
   ],
   "source": [
    "# Read vectorized features from the data files\n",
    "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"vMalConv/\")\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c324a91e-c05f-439d-92b8-65161a29c2b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering unlabeled data...\n",
      "After filtering, X_train shape: (600000, 2381)\n",
      "After filtering, y_train shape: (600000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering unlabeled data...\")\n",
    "labelrows = (y_train != -1)\n",
    "X_train = X_train[labelrows]\n",
    "y_train = y_train[labelrows]\n",
    "\n",
    "print(f\"After filtering, X_train shape: {X_train.shape}\")\n",
    "print(f\"After filtering, y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281cf4f1-8921-48b0-9c77-d3e6523efafc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling the dataset...\n",
      "Original class distribution: 0.5000 malware, 0.5000 benign\n",
      "Sampling 45000 malware and 45000 benign samples\n",
      "Sampled X_train shape: (90000, 2381)\n",
      "Sampled y_train shape: (90000,)\n",
      "Sampled X_test shape: (25000, 2381)\n",
      "Sampled y_test shape: (25000,)\n",
      "Sampled class distribution: 0.5000 malware, 0.5000 benign\n"
     ]
    }
   ],
   "source": [
    "# Sample the dataset to speed up experiments\n",
    "print(\"Sampling the dataset...\")\n",
    "\n",
    "# Set sample sizes\n",
    "train_sample_size = 90000  # Adjust based on your computational resources\n",
    "test_sample_size = 25000\n",
    "\n",
    "# Sample training data, preserving class distribution\n",
    "malware_indices = np.where(y_train == 1)[0]\n",
    "benign_indices = np.where(y_train == 0)[0]\n",
    "\n",
    "malware_ratio = len(malware_indices) / len(y_train)\n",
    "malware_sample_size = int(train_sample_size * malware_ratio)\n",
    "benign_sample_size = train_sample_size - malware_sample_size\n",
    "\n",
    "print(f\"Original class distribution: {malware_ratio:.4f} malware, {1-malware_ratio:.4f} benign\")\n",
    "print(f\"Sampling {malware_sample_size} malware and {benign_sample_size} benign samples\")\n",
    "\n",
    "# Randomly sample indices\n",
    "np.random.seed(42)  # For reproducibility\n",
    "sampled_malware_indices = np.random.choice(malware_indices, size=malware_sample_size, replace=False)\n",
    "sampled_benign_indices = np.random.choice(benign_indices, size=benign_sample_size, replace=False)\n",
    "\n",
    "# Combine indices\n",
    "sampled_indices = np.concatenate([sampled_malware_indices, sampled_benign_indices])\n",
    "np.random.shuffle(sampled_indices)\n",
    "\n",
    "# Create sampled training datasets\n",
    "X_train = X_train[sampled_indices]\n",
    "y_train = y_train[sampled_indices]\n",
    "\n",
    "# Check test dataset for unlabeled entries\n",
    "test_labelrows = (y_test != -1)\n",
    "X_test = X_test[test_labelrows]\n",
    "y_test = y_test[test_labelrows]\n",
    "\n",
    "# Sample test data similarly\n",
    "test_malware_indices = np.where(y_test == 1)[0]\n",
    "test_benign_indices = np.where(y_test == 0)[0]\n",
    "\n",
    "test_malware_ratio = len(test_malware_indices) / len(y_test)\n",
    "test_malware_sample_size = int(test_sample_size * test_malware_ratio)\n",
    "test_benign_sample_size = test_sample_size - test_malware_sample_size\n",
    "\n",
    "sampled_test_malware = np.random.choice(test_malware_indices, size=test_malware_sample_size, replace=False)\n",
    "sampled_test_benign = np.random.choice(test_benign_indices, size=test_benign_sample_size, replace=False)\n",
    "\n",
    "sampled_test_indices = np.concatenate([sampled_test_malware, sampled_test_benign])\n",
    "np.random.shuffle(sampled_test_indices)\n",
    "\n",
    "X_test = X_test[sampled_test_indices]\n",
    "y_test = y_test[sampled_test_indices]\n",
    "\n",
    "# Print sampled dataset shapes\n",
    "print(f\"Sampled X_train shape: {X_train.shape}\")\n",
    "print(f\"Sampled y_train shape: {y_train.shape}\")\n",
    "print(f\"Sampled X_test shape: {X_test.shape}\")\n",
    "print(f\"Sampled y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Check final class distribution\n",
    "sampled_malware_count = np.sum(y_train == 1)\n",
    "sampled_benign_count = np.sum(y_train == 0)\n",
    "sampled_ratio = sampled_malware_count / len(y_train)\n",
    "print(f\"Sampled class distribution: {sampled_ratio:.4f} malware, {1-sampled_ratio:.4f} benign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5bda0d3-eb06-471c-a5c0-6c559ba24880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing features...\n",
      "Processed batch 1\n",
      "Feature standardization complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "print(\"Standardizing features...\")\n",
    "mms = StandardScaler()\n",
    "\n",
    "# Partial fit in batches to avoid memory issues\n",
    "for x in range(0, len(X_train), 100000):\n",
    "    end_idx = min(x + 100000, len(X_train)) \n",
    "    mms.partial_fit(X_train[x:end_idx])\n",
    "    print(f\"Processed batch {x//100000 + 1}\")\n",
    "\n",
    "# Transform the training data\n",
    "X_train = mms.transform(X_train)\n",
    "print(\"Feature standardization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ecd2308-f3d8-4409-a8aa-e421bbc03ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved as scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Save the fitted scaler to a file\n",
    "joblib.dump(mms, \"scaler.joblib\")\n",
    "print(\"Scaler saved as scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67de4528-a6c0-43b1-85d3-a767127260a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping data...\n",
      "Reshaped X_train shape: (90000, 1, 2381)\n",
      "Reshaped y_train shape: (90000, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape to create appropriate format for the model\n",
    "print(\"Reshaping data...\")\n",
    "X_train = np.reshape(X_train, (-1, 1, X_train.shape[1]))\n",
    "y_train = np.reshape(y_train, (-1, 1, 1))\n",
    "print(f\"Reshaped X_train shape: {X_train.shape}\")\n",
    "print(f\"Reshaped y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1eaca1ad-13a9-47fb-81f1-f4591dc00889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MalConv(\n",
      "  (fc1): Linear(in_features=2381, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MalConv(nn.Module):\n",
    "    def __init__(self, input_size=2381, hidden_size=128, output_dim=1):\n",
    "        super(MalConv, self).__init__()\n",
    "        \n",
    "        # For EMBER feature vectors, we don't need an embedding layer\n",
    "        # Instead, we'll use fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_dim)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Remove the channel dimension (batch_size, 1, features) -> (batch_size, features)\n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        # First layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MalConv(input_size=X_train.shape[2])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f48266db-f218-4998-8837-aa3f2c541848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing PyTorch datasets...\n",
      "DataLoaders created successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Preparing PyTorch datasets...\")\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_split, y_train_split)\n",
    "val_dataset = TensorDataset(X_val_split, y_val_split)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 128  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"DataLoaders created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "622288e0-9134-432f-8c4f-8cee1f7884f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/20], Step [10/563], Loss: 0.1737\n",
      "Epoch [1/20], Step [20/563], Loss: 0.1432\n",
      "Epoch [1/20], Step [30/563], Loss: 0.1446\n",
      "Epoch [1/20], Step [40/563], Loss: 0.1231\n",
      "Epoch [1/20], Step [50/563], Loss: 0.1101\n",
      "Epoch [1/20], Step [60/563], Loss: 0.1259\n",
      "Epoch [1/20], Step [70/563], Loss: 0.0915\n",
      "Epoch [1/20], Step [80/563], Loss: 0.1329\n",
      "Epoch [1/20], Step [90/563], Loss: 0.1095\n",
      "Epoch [1/20], Step [100/563], Loss: 0.1064\n",
      "Epoch [1/20], Step [110/563], Loss: 0.0794\n",
      "Epoch [1/20], Step [120/563], Loss: 0.1160\n",
      "Epoch [1/20], Step [130/563], Loss: 0.1092\n",
      "Epoch [1/20], Step [140/563], Loss: 0.0635\n",
      "Epoch [1/20], Step [150/563], Loss: 0.0831\n",
      "Epoch [1/20], Step [160/563], Loss: 0.0792\n",
      "Epoch [1/20], Step [170/563], Loss: 0.0967\n",
      "Epoch [1/20], Step [180/563], Loss: 0.0787\n",
      "Epoch [1/20], Step [190/563], Loss: 0.0908\n",
      "Epoch [1/20], Step [200/563], Loss: 0.0911\n",
      "Epoch [1/20], Step [210/563], Loss: 0.1067\n",
      "Epoch [1/20], Step [220/563], Loss: 0.1050\n",
      "Epoch [1/20], Step [230/563], Loss: 0.0904\n",
      "Epoch [1/20], Step [240/563], Loss: 0.0472\n",
      "Epoch [1/20], Step [250/563], Loss: 0.1086\n",
      "Epoch [1/20], Step [260/563], Loss: 0.0935\n",
      "Epoch [1/20], Step [270/563], Loss: 0.0808\n",
      "Epoch [1/20], Step [280/563], Loss: 0.0855\n",
      "Epoch [1/20], Step [290/563], Loss: 0.0970\n",
      "Epoch [1/20], Step [300/563], Loss: 0.1172\n",
      "Epoch [1/20], Step [310/563], Loss: 0.0958\n",
      "Epoch [1/20], Step [320/563], Loss: 0.0867\n",
      "Epoch [1/20], Step [330/563], Loss: 0.1225\n",
      "Epoch [1/20], Step [340/563], Loss: 0.1241\n",
      "Epoch [1/20], Step [350/563], Loss: 0.1011\n",
      "Epoch [1/20], Step [360/563], Loss: 0.1055\n",
      "Epoch [1/20], Step [370/563], Loss: 0.0940\n",
      "Epoch [1/20], Step [380/563], Loss: 0.1546\n",
      "Epoch [1/20], Step [390/563], Loss: 0.0773\n",
      "Epoch [1/20], Step [400/563], Loss: 0.0722\n",
      "Epoch [1/20], Step [410/563], Loss: 0.0868\n",
      "Epoch [1/20], Step [420/563], Loss: 0.0901\n",
      "Epoch [1/20], Step [430/563], Loss: 0.1057\n",
      "Epoch [1/20], Step [440/563], Loss: 0.0611\n",
      "Epoch [1/20], Step [450/563], Loss: 0.0691\n",
      "Epoch [1/20], Step [460/563], Loss: 0.1106\n",
      "Epoch [1/20], Step [470/563], Loss: 0.0561\n",
      "Epoch [1/20], Step [480/563], Loss: 0.0682\n",
      "Epoch [1/20], Step [490/563], Loss: 0.0768\n",
      "Epoch [1/20], Step [500/563], Loss: 0.1248\n",
      "Epoch [1/20], Step [510/563], Loss: 0.0980\n",
      "Epoch [1/20], Step [520/563], Loss: 0.0892\n",
      "Epoch [1/20], Step [530/563], Loss: 0.0833\n",
      "Epoch [1/20], Step [540/563], Loss: 0.1065\n",
      "Epoch [1/20], Step [550/563], Loss: 0.0621\n",
      "Epoch [1/20], Step [560/563], Loss: 0.0835\n",
      "Epoch 1, Training_Loss: 0.0993, Time: 3.58s\n",
      "Validation_Loss: 0.0778, Validation_Accuracy: 89.74%\n",
      "Epoch [2/20], Step [10/563], Loss: 0.0936\n",
      "Epoch [2/20], Step [20/563], Loss: 0.0589\n",
      "Epoch [2/20], Step [30/563], Loss: 0.0816\n",
      "Epoch [2/20], Step [40/563], Loss: 0.0521\n",
      "Epoch [2/20], Step [50/563], Loss: 0.0846\n",
      "Epoch [2/20], Step [60/563], Loss: 0.1146\n",
      "Epoch [2/20], Step [70/563], Loss: 0.0782\n",
      "Epoch [2/20], Step [80/563], Loss: 0.0954\n",
      "Epoch [2/20], Step [90/563], Loss: 0.0702\n",
      "Epoch [2/20], Step [100/563], Loss: 0.1043\n",
      "Epoch [2/20], Step [110/563], Loss: 0.1030\n",
      "Epoch [2/20], Step [120/563], Loss: 0.0806\n",
      "Epoch [2/20], Step [130/563], Loss: 0.0832\n",
      "Epoch [2/20], Step [140/563], Loss: 0.0595\n",
      "Epoch [2/20], Step [150/563], Loss: 0.0537\n",
      "Epoch [2/20], Step [160/563], Loss: 0.1028\n",
      "Epoch [2/20], Step [170/563], Loss: 0.0856\n",
      "Epoch [2/20], Step [180/563], Loss: 0.1012\n",
      "Epoch [2/20], Step [190/563], Loss: 0.1222\n",
      "Epoch [2/20], Step [200/563], Loss: 0.0877\n",
      "Epoch [2/20], Step [210/563], Loss: 0.0777\n",
      "Epoch [2/20], Step [220/563], Loss: 0.0739\n",
      "Epoch [2/20], Step [230/563], Loss: 0.0883\n",
      "Epoch [2/20], Step [240/563], Loss: 0.0624\n",
      "Epoch [2/20], Step [250/563], Loss: 0.0818\n",
      "Epoch [2/20], Step [260/563], Loss: 0.0568\n",
      "Epoch [2/20], Step [270/563], Loss: 0.1131\n",
      "Epoch [2/20], Step [280/563], Loss: 0.1046\n",
      "Epoch [2/20], Step [290/563], Loss: 0.0911\n",
      "Epoch [2/20], Step [300/563], Loss: 0.1079\n",
      "Epoch [2/20], Step [310/563], Loss: 0.0738\n",
      "Epoch [2/20], Step [320/563], Loss: 0.0547\n",
      "Epoch [2/20], Step [330/563], Loss: 0.0999\n",
      "Epoch [2/20], Step [340/563], Loss: 0.0619\n",
      "Epoch [2/20], Step [350/563], Loss: 0.0593\n",
      "Epoch [2/20], Step [360/563], Loss: 0.0625\n",
      "Epoch [2/20], Step [370/563], Loss: 0.0959\n",
      "Epoch [2/20], Step [380/563], Loss: 0.0481\n",
      "Epoch [2/20], Step [390/563], Loss: 0.0750\n",
      "Epoch [2/20], Step [400/563], Loss: 0.0624\n",
      "Epoch [2/20], Step [410/563], Loss: 0.0949\n",
      "Epoch [2/20], Step [420/563], Loss: 0.1007\n",
      "Epoch [2/20], Step [430/563], Loss: 0.0823\n",
      "Epoch [2/20], Step [440/563], Loss: 0.0937\n",
      "Epoch [2/20], Step [450/563], Loss: 0.0867\n",
      "Epoch [2/20], Step [460/563], Loss: 0.1088\n",
      "Epoch [2/20], Step [470/563], Loss: 0.0601\n",
      "Epoch [2/20], Step [480/563], Loss: 0.0786\n",
      "Epoch [2/20], Step [490/563], Loss: 0.1183\n",
      "Epoch [2/20], Step [500/563], Loss: 0.0626\n",
      "Epoch [2/20], Step [510/563], Loss: 0.0656\n",
      "Epoch [2/20], Step [520/563], Loss: 0.0780\n",
      "Epoch [2/20], Step [530/563], Loss: 0.0523\n",
      "Epoch [2/20], Step [540/563], Loss: 0.1014\n",
      "Epoch [2/20], Step [550/563], Loss: 0.0655\n",
      "Epoch [2/20], Step [560/563], Loss: 0.0861\n",
      "Epoch 2, Training_Loss: 0.0809, Time: 2.25s\n",
      "Validation_Loss: 0.0778, Validation_Accuracy: 90.13%\n",
      "Epoch [3/20], Step [10/563], Loss: 0.0519\n",
      "Epoch [3/20], Step [20/563], Loss: 0.0476\n",
      "Epoch [3/20], Step [30/563], Loss: 0.0688\n",
      "Epoch [3/20], Step [40/563], Loss: 0.0824\n",
      "Epoch [3/20], Step [50/563], Loss: 0.0689\n",
      "Epoch [3/20], Step [60/563], Loss: 0.0534\n",
      "Epoch [3/20], Step [70/563], Loss: 0.0826\n",
      "Epoch [3/20], Step [80/563], Loss: 0.0531\n",
      "Epoch [3/20], Step [90/563], Loss: 0.0787\n",
      "Epoch [3/20], Step [100/563], Loss: 0.0608\n",
      "Epoch [3/20], Step [110/563], Loss: 0.0845\n",
      "Epoch [3/20], Step [120/563], Loss: 0.0709\n",
      "Epoch [3/20], Step [130/563], Loss: 0.0456\n",
      "Epoch [3/20], Step [140/563], Loss: 0.0984\n",
      "Epoch [3/20], Step [150/563], Loss: 0.0921\n",
      "Epoch [3/20], Step [160/563], Loss: 0.0522\n",
      "Epoch [3/20], Step [170/563], Loss: 0.0708\n",
      "Epoch [3/20], Step [180/563], Loss: 0.0629\n",
      "Epoch [3/20], Step [190/563], Loss: 0.0930\n",
      "Epoch [3/20], Step [200/563], Loss: 0.0824\n",
      "Epoch [3/20], Step [210/563], Loss: 0.0812\n",
      "Epoch [3/20], Step [220/563], Loss: 0.0543\n",
      "Epoch [3/20], Step [230/563], Loss: 0.0832\n",
      "Epoch [3/20], Step [240/563], Loss: 0.0934\n",
      "Epoch [3/20], Step [250/563], Loss: 0.1311\n",
      "Epoch [3/20], Step [260/563], Loss: 0.0709\n",
      "Epoch [3/20], Step [270/563], Loss: 0.0613\n",
      "Epoch [3/20], Step [280/563], Loss: 0.0531\n",
      "Epoch [3/20], Step [290/563], Loss: 0.0809\n",
      "Epoch [3/20], Step [300/563], Loss: 0.0617\n",
      "Epoch [3/20], Step [310/563], Loss: 0.0594\n",
      "Epoch [3/20], Step [320/563], Loss: 0.0835\n",
      "Epoch [3/20], Step [330/563], Loss: 0.0877\n",
      "Epoch [3/20], Step [340/563], Loss: 0.0805\n",
      "Epoch [3/20], Step [350/563], Loss: 0.0911\n",
      "Epoch [3/20], Step [360/563], Loss: 0.0441\n",
      "Epoch [3/20], Step [370/563], Loss: 0.0591\n",
      "Epoch [3/20], Step [380/563], Loss: 0.0938\n",
      "Epoch [3/20], Step [390/563], Loss: 0.0954\n",
      "Epoch [3/20], Step [400/563], Loss: 0.0789\n",
      "Epoch [3/20], Step [410/563], Loss: 0.0601\n",
      "Epoch [3/20], Step [420/563], Loss: 0.0438\n",
      "Epoch [3/20], Step [430/563], Loss: 0.0912\n",
      "Epoch [3/20], Step [440/563], Loss: 0.0791\n",
      "Epoch [3/20], Step [450/563], Loss: 0.0683\n",
      "Epoch [3/20], Step [460/563], Loss: 0.0717\n",
      "Epoch [3/20], Step [470/563], Loss: 0.0860\n",
      "Epoch [3/20], Step [480/563], Loss: 0.0843\n",
      "Epoch [3/20], Step [490/563], Loss: 0.0789\n",
      "Epoch [3/20], Step [500/563], Loss: 0.0863\n",
      "Epoch [3/20], Step [510/563], Loss: 0.0432\n",
      "Epoch [3/20], Step [520/563], Loss: 0.0740\n",
      "Epoch [3/20], Step [530/563], Loss: 0.0976\n",
      "Epoch [3/20], Step [540/563], Loss: 0.0889\n",
      "Epoch [3/20], Step [550/563], Loss: 0.0647\n",
      "Epoch [3/20], Step [560/563], Loss: 0.0915\n",
      "Epoch 3, Training_Loss: 0.0754, Time: 2.22s\n",
      "Validation_Loss: 0.0676, Validation_Accuracy: 91.39%\n",
      "Epoch [4/20], Step [10/563], Loss: 0.0730\n",
      "Epoch [4/20], Step [20/563], Loss: 0.0541\n",
      "Epoch [4/20], Step [30/563], Loss: 0.0376\n",
      "Epoch [4/20], Step [40/563], Loss: 0.0733\n",
      "Epoch [4/20], Step [50/563], Loss: 0.0773\n",
      "Epoch [4/20], Step [60/563], Loss: 0.0465\n",
      "Epoch [4/20], Step [70/563], Loss: 0.0954\n",
      "Epoch [4/20], Step [80/563], Loss: 0.0971\n",
      "Epoch [4/20], Step [90/563], Loss: 0.0918\n",
      "Epoch [4/20], Step [100/563], Loss: 0.0934\n",
      "Epoch [4/20], Step [110/563], Loss: 0.0975\n",
      "Epoch [4/20], Step [120/563], Loss: 0.0847\n",
      "Epoch [4/20], Step [130/563], Loss: 0.0793\n",
      "Epoch [4/20], Step [140/563], Loss: 0.0718\n",
      "Epoch [4/20], Step [150/563], Loss: 0.0653\n",
      "Epoch [4/20], Step [160/563], Loss: 0.0550\n",
      "Epoch [4/20], Step [170/563], Loss: 0.0897\n",
      "Epoch [4/20], Step [180/563], Loss: 0.0602\n",
      "Epoch [4/20], Step [190/563], Loss: 0.0500\n",
      "Epoch [4/20], Step [200/563], Loss: 0.0677\n",
      "Epoch [4/20], Step [210/563], Loss: 0.0745\n",
      "Epoch [4/20], Step [220/563], Loss: 0.0739\n",
      "Epoch [4/20], Step [230/563], Loss: 0.0703\n",
      "Epoch [4/20], Step [240/563], Loss: 0.0567\n",
      "Epoch [4/20], Step [250/563], Loss: 0.0833\n",
      "Epoch [4/20], Step [260/563], Loss: 0.0579\n",
      "Epoch [4/20], Step [270/563], Loss: 0.0551\n",
      "Epoch [4/20], Step [280/563], Loss: 0.0558\n",
      "Epoch [4/20], Step [290/563], Loss: 0.0658\n",
      "Epoch [4/20], Step [300/563], Loss: 0.0923\n",
      "Epoch [4/20], Step [310/563], Loss: 0.0621\n",
      "Epoch [4/20], Step [320/563], Loss: 0.0994\n",
      "Epoch [4/20], Step [330/563], Loss: 0.0576\n",
      "Epoch [4/20], Step [340/563], Loss: 0.0478\n",
      "Epoch [4/20], Step [350/563], Loss: 0.0481\n",
      "Epoch [4/20], Step [360/563], Loss: 0.0745\n",
      "Epoch [4/20], Step [370/563], Loss: 0.0939\n",
      "Epoch [4/20], Step [380/563], Loss: 0.0573\n",
      "Epoch [4/20], Step [390/563], Loss: 0.0497\n",
      "Epoch [4/20], Step [400/563], Loss: 0.0595\n",
      "Epoch [4/20], Step [410/563], Loss: 0.0674\n",
      "Epoch [4/20], Step [420/563], Loss: 0.0971\n",
      "Epoch [4/20], Step [430/563], Loss: 0.1245\n",
      "Epoch [4/20], Step [440/563], Loss: 0.0502\n",
      "Epoch [4/20], Step [450/563], Loss: 0.0517\n",
      "Epoch [4/20], Step [460/563], Loss: 0.0715\n",
      "Epoch [4/20], Step [470/563], Loss: 0.0524\n",
      "Epoch [4/20], Step [480/563], Loss: 0.0739\n",
      "Epoch [4/20], Step [490/563], Loss: 0.0639\n",
      "Epoch [4/20], Step [500/563], Loss: 0.1030\n",
      "Epoch [4/20], Step [510/563], Loss: 0.0528\n",
      "Epoch [4/20], Step [520/563], Loss: 0.0902\n",
      "Epoch [4/20], Step [530/563], Loss: 0.0583\n",
      "Epoch [4/20], Step [540/563], Loss: 0.1000\n",
      "Epoch [4/20], Step [550/563], Loss: 0.1289\n",
      "Epoch [4/20], Step [560/563], Loss: 0.0826\n",
      "Epoch 4, Training_Loss: 0.0723, Time: 2.23s\n",
      "Validation_Loss: 0.0675, Validation_Accuracy: 91.59%\n",
      "Epoch [5/20], Step [10/563], Loss: 0.0465\n",
      "Epoch [5/20], Step [20/563], Loss: 0.0625\n",
      "Epoch [5/20], Step [30/563], Loss: 0.0724\n",
      "Epoch [5/20], Step [40/563], Loss: 0.0376\n",
      "Epoch [5/20], Step [50/563], Loss: 0.0836\n",
      "Epoch [5/20], Step [60/563], Loss: 0.0537\n",
      "Epoch [5/20], Step [70/563], Loss: 0.1162\n",
      "Epoch [5/20], Step [80/563], Loss: 0.0920\n",
      "Epoch [5/20], Step [90/563], Loss: 0.0892\n",
      "Epoch [5/20], Step [100/563], Loss: 0.0570\n",
      "Epoch [5/20], Step [110/563], Loss: 0.0512\n",
      "Epoch [5/20], Step [120/563], Loss: 0.0724\n",
      "Epoch [5/20], Step [130/563], Loss: 0.0862\n",
      "Epoch [5/20], Step [140/563], Loss: 0.0871\n",
      "Epoch [5/20], Step [150/563], Loss: 0.0697\n",
      "Epoch [5/20], Step [160/563], Loss: 0.0613\n",
      "Epoch [5/20], Step [170/563], Loss: 0.0686\n",
      "Epoch [5/20], Step [180/563], Loss: 0.1006\n",
      "Epoch [5/20], Step [190/563], Loss: 0.0611\n",
      "Epoch [5/20], Step [200/563], Loss: 0.0512\n",
      "Epoch [5/20], Step [210/563], Loss: 0.0339\n",
      "Epoch [5/20], Step [220/563], Loss: 0.0726\n",
      "Epoch [5/20], Step [230/563], Loss: 0.0635\n",
      "Epoch [5/20], Step [240/563], Loss: 0.0840\n",
      "Epoch [5/20], Step [250/563], Loss: 0.0701\n",
      "Epoch [5/20], Step [260/563], Loss: 0.0798\n",
      "Epoch [5/20], Step [270/563], Loss: 0.0928\n",
      "Epoch [5/20], Step [280/563], Loss: 0.0820\n",
      "Epoch [5/20], Step [290/563], Loss: 0.0510\n",
      "Epoch [5/20], Step [300/563], Loss: 0.0855\n",
      "Epoch [5/20], Step [310/563], Loss: 0.0533\n",
      "Epoch [5/20], Step [320/563], Loss: 0.0865\n",
      "Epoch [5/20], Step [330/563], Loss: 0.0736\n",
      "Epoch [5/20], Step [340/563], Loss: 0.0586\n",
      "Epoch [5/20], Step [350/563], Loss: 0.0454\n",
      "Epoch [5/20], Step [360/563], Loss: 0.1243\n",
      "Epoch [5/20], Step [370/563], Loss: 0.1020\n",
      "Epoch [5/20], Step [380/563], Loss: 0.0385\n",
      "Epoch [5/20], Step [390/563], Loss: 0.0399\n",
      "Epoch [5/20], Step [400/563], Loss: 0.0747\n",
      "Epoch [5/20], Step [410/563], Loss: 0.0660\n",
      "Epoch [5/20], Step [420/563], Loss: 0.0388\n",
      "Epoch [5/20], Step [430/563], Loss: 0.0774\n",
      "Epoch [5/20], Step [440/563], Loss: 0.0430\n",
      "Epoch [5/20], Step [450/563], Loss: 0.1124\n",
      "Epoch [5/20], Step [460/563], Loss: 0.0679\n",
      "Epoch [5/20], Step [470/563], Loss: 0.0671\n",
      "Epoch [5/20], Step [480/563], Loss: 0.0672\n",
      "Epoch [5/20], Step [490/563], Loss: 0.0754\n",
      "Epoch [5/20], Step [500/563], Loss: 0.0641\n",
      "Epoch [5/20], Step [510/563], Loss: 0.0467\n",
      "Epoch [5/20], Step [520/563], Loss: 0.0938\n",
      "Epoch [5/20], Step [530/563], Loss: 0.0916\n",
      "Epoch [5/20], Step [540/563], Loss: 0.0663\n",
      "Epoch [5/20], Step [550/563], Loss: 0.0374\n",
      "Epoch [5/20], Step [560/563], Loss: 0.0934\n",
      "Epoch 5, Training_Loss: 0.0717, Time: 2.26s\n",
      "Validation_Loss: 0.0669, Validation_Accuracy: 91.49%\n",
      "Model checkpoint saved to /home/ec2-user/SageMaker/model_epoch_5.pt\n",
      "Epoch [6/20], Step [10/563], Loss: 0.0612\n",
      "Epoch [6/20], Step [20/563], Loss: 0.0625\n",
      "Epoch [6/20], Step [30/563], Loss: 0.0546\n",
      "Epoch [6/20], Step [40/563], Loss: 0.0602\n",
      "Epoch [6/20], Step [50/563], Loss: 0.0752\n",
      "Epoch [6/20], Step [60/563], Loss: 0.0573\n",
      "Epoch [6/20], Step [70/563], Loss: 0.0783\n",
      "Epoch [6/20], Step [80/563], Loss: 0.0895\n",
      "Epoch [6/20], Step [90/563], Loss: 0.0530\n",
      "Epoch [6/20], Step [100/563], Loss: 0.0998\n",
      "Epoch [6/20], Step [110/563], Loss: 0.0982\n",
      "Epoch [6/20], Step [120/563], Loss: 0.0885\n",
      "Epoch [6/20], Step [130/563], Loss: 0.0787\n",
      "Epoch [6/20], Step [140/563], Loss: 0.0893\n",
      "Epoch [6/20], Step [150/563], Loss: 0.0657\n",
      "Epoch [6/20], Step [160/563], Loss: 0.0891\n",
      "Epoch [6/20], Step [170/563], Loss: 0.0595\n",
      "Epoch [6/20], Step [180/563], Loss: 0.0604\n",
      "Epoch [6/20], Step [190/563], Loss: 0.0823\n",
      "Epoch [6/20], Step [200/563], Loss: 0.0467\n",
      "Epoch [6/20], Step [210/563], Loss: 0.0426\n",
      "Epoch [6/20], Step [220/563], Loss: 0.0564\n",
      "Epoch [6/20], Step [230/563], Loss: 0.0550\n",
      "Epoch [6/20], Step [240/563], Loss: 0.0651\n",
      "Epoch [6/20], Step [250/563], Loss: 0.1171\n",
      "Epoch [6/20], Step [260/563], Loss: 0.0861\n",
      "Epoch [6/20], Step [270/563], Loss: 0.0780\n",
      "Epoch [6/20], Step [280/563], Loss: 0.0654\n",
      "Epoch [6/20], Step [290/563], Loss: 0.0633\n",
      "Epoch [6/20], Step [300/563], Loss: 0.0989\n",
      "Epoch [6/20], Step [310/563], Loss: 0.0710\n",
      "Epoch [6/20], Step [320/563], Loss: 0.0716\n",
      "Epoch [6/20], Step [330/563], Loss: 0.0776\n",
      "Epoch [6/20], Step [340/563], Loss: 0.0606\n",
      "Epoch [6/20], Step [350/563], Loss: 0.0738\n",
      "Epoch [6/20], Step [360/563], Loss: 0.0539\n",
      "Epoch [6/20], Step [370/563], Loss: 0.0776\n",
      "Epoch [6/20], Step [380/563], Loss: 0.0741\n",
      "Epoch [6/20], Step [390/563], Loss: 0.0507\n",
      "Epoch [6/20], Step [400/563], Loss: 0.0729\n",
      "Epoch [6/20], Step [410/563], Loss: 0.0780\n",
      "Epoch [6/20], Step [420/563], Loss: 0.0914\n",
      "Epoch [6/20], Step [430/563], Loss: 0.0847\n",
      "Epoch [6/20], Step [440/563], Loss: 0.0846\n",
      "Epoch [6/20], Step [450/563], Loss: 0.0713\n",
      "Epoch [6/20], Step [460/563], Loss: 0.0641\n",
      "Epoch [6/20], Step [470/563], Loss: 0.0621\n",
      "Epoch [6/20], Step [480/563], Loss: 0.0679\n",
      "Epoch [6/20], Step [490/563], Loss: 0.0942\n",
      "Epoch [6/20], Step [500/563], Loss: 0.0948\n",
      "Epoch [6/20], Step [510/563], Loss: 0.0786\n",
      "Epoch [6/20], Step [520/563], Loss: 0.0603\n",
      "Epoch [6/20], Step [530/563], Loss: 0.0421\n",
      "Epoch [6/20], Step [540/563], Loss: 0.0784\n",
      "Epoch [6/20], Step [550/563], Loss: 0.0763\n",
      "Epoch [6/20], Step [560/563], Loss: 0.0641\n",
      "Epoch 6, Training_Loss: 0.0705, Time: 2.27s\n",
      "Validation_Loss: 0.0664, Validation_Accuracy: 91.68%\n",
      "Epoch [7/20], Step [10/563], Loss: 0.0615\n",
      "Epoch [7/20], Step [20/563], Loss: 0.0734\n",
      "Epoch [7/20], Step [30/563], Loss: 0.0682\n",
      "Epoch [7/20], Step [40/563], Loss: 0.0640\n",
      "Epoch [7/20], Step [50/563], Loss: 0.0853\n",
      "Epoch [7/20], Step [60/563], Loss: 0.0918\n",
      "Epoch [7/20], Step [70/563], Loss: 0.0783\n",
      "Epoch [7/20], Step [80/563], Loss: 0.0523\n",
      "Epoch [7/20], Step [90/563], Loss: 0.0526\n",
      "Epoch [7/20], Step [100/563], Loss: 0.0719\n",
      "Epoch [7/20], Step [110/563], Loss: 0.0903\n",
      "Epoch [7/20], Step [120/563], Loss: 0.1007\n",
      "Epoch [7/20], Step [130/563], Loss: 0.0668\n",
      "Epoch [7/20], Step [140/563], Loss: 0.0545\n",
      "Epoch [7/20], Step [150/563], Loss: 0.0430\n",
      "Epoch [7/20], Step [160/563], Loss: 0.0494\n",
      "Epoch [7/20], Step [170/563], Loss: 0.0615\n",
      "Epoch [7/20], Step [180/563], Loss: 0.0914\n",
      "Epoch [7/20], Step [190/563], Loss: 0.0482\n",
      "Epoch [7/20], Step [200/563], Loss: 0.1007\n",
      "Epoch [7/20], Step [210/563], Loss: 0.0455\n",
      "Epoch [7/20], Step [220/563], Loss: 0.0652\n",
      "Epoch [7/20], Step [230/563], Loss: 0.0676\n",
      "Epoch [7/20], Step [240/563], Loss: 0.0607\n",
      "Epoch [7/20], Step [250/563], Loss: 0.0647\n",
      "Epoch [7/20], Step [260/563], Loss: 0.0762\n",
      "Epoch [7/20], Step [270/563], Loss: 0.0696\n",
      "Epoch [7/20], Step [280/563], Loss: 0.0480\n",
      "Epoch [7/20], Step [290/563], Loss: 0.0535\n",
      "Epoch [7/20], Step [300/563], Loss: 0.0613\n",
      "Epoch [7/20], Step [310/563], Loss: 0.0734\n",
      "Epoch [7/20], Step [320/563], Loss: 0.0845\n",
      "Epoch [7/20], Step [330/563], Loss: 0.0602\n",
      "Epoch [7/20], Step [340/563], Loss: 0.0558\n",
      "Epoch [7/20], Step [350/563], Loss: 0.0960\n",
      "Epoch [7/20], Step [360/563], Loss: 0.0762\n",
      "Epoch [7/20], Step [370/563], Loss: 0.0928\n",
      "Epoch [7/20], Step [380/563], Loss: 0.0940\n",
      "Epoch [7/20], Step [390/563], Loss: 0.0560\n",
      "Epoch [7/20], Step [400/563], Loss: 0.0735\n",
      "Epoch [7/20], Step [410/563], Loss: 0.0791\n",
      "Epoch [7/20], Step [420/563], Loss: 0.0883\n",
      "Epoch [7/20], Step [430/563], Loss: 0.0480\n",
      "Epoch [7/20], Step [440/563], Loss: 0.0671\n",
      "Epoch [7/20], Step [450/563], Loss: 0.0820\n",
      "Epoch [7/20], Step [460/563], Loss: 0.0617\n",
      "Epoch [7/20], Step [470/563], Loss: 0.0943\n",
      "Epoch [7/20], Step [480/563], Loss: 0.0977\n",
      "Epoch [7/20], Step [490/563], Loss: 0.0674\n",
      "Epoch [7/20], Step [500/563], Loss: 0.0746\n",
      "Epoch [7/20], Step [510/563], Loss: 0.0775\n",
      "Epoch [7/20], Step [520/563], Loss: 0.0619\n",
      "Epoch [7/20], Step [530/563], Loss: 0.0578\n",
      "Epoch [7/20], Step [540/563], Loss: 0.0635\n",
      "Epoch [7/20], Step [550/563], Loss: 0.0603\n",
      "Epoch [7/20], Step [560/563], Loss: 0.0970\n",
      "Epoch 7, Training_Loss: 0.0688, Time: 2.23s\n",
      "Validation_Loss: 0.0648, Validation_Accuracy: 91.98%\n",
      "Epoch [8/20], Step [10/563], Loss: 0.0963\n",
      "Epoch [8/20], Step [20/563], Loss: 0.0626\n",
      "Epoch [8/20], Step [30/563], Loss: 0.0490\n",
      "Epoch [8/20], Step [40/563], Loss: 0.0636\n",
      "Epoch [8/20], Step [50/563], Loss: 0.0728\n",
      "Epoch [8/20], Step [60/563], Loss: 0.0537\n",
      "Epoch [8/20], Step [70/563], Loss: 0.0821\n",
      "Epoch [8/20], Step [80/563], Loss: 0.0799\n",
      "Epoch [8/20], Step [90/563], Loss: 0.0864\n",
      "Epoch [8/20], Step [100/563], Loss: 0.0817\n",
      "Epoch [8/20], Step [110/563], Loss: 0.0890\n",
      "Epoch [8/20], Step [120/563], Loss: 0.0417\n",
      "Epoch [8/20], Step [130/563], Loss: 0.1134\n",
      "Epoch [8/20], Step [140/563], Loss: 0.0531\n",
      "Epoch [8/20], Step [150/563], Loss: 0.0556\n",
      "Epoch [8/20], Step [160/563], Loss: 0.0512\n",
      "Epoch [8/20], Step [170/563], Loss: 0.0495\n",
      "Epoch [8/20], Step [180/563], Loss: 0.0615\n",
      "Epoch [8/20], Step [190/563], Loss: 0.0771\n",
      "Epoch [8/20], Step [200/563], Loss: 0.0710\n",
      "Epoch [8/20], Step [210/563], Loss: 0.0815\n",
      "Epoch [8/20], Step [220/563], Loss: 0.0373\n",
      "Epoch [8/20], Step [230/563], Loss: 0.0566\n",
      "Epoch [8/20], Step [240/563], Loss: 0.0322\n",
      "Epoch [8/20], Step [250/563], Loss: 0.0946\n",
      "Epoch [8/20], Step [260/563], Loss: 0.0787\n",
      "Epoch [8/20], Step [270/563], Loss: 0.0638\n",
      "Epoch [8/20], Step [280/563], Loss: 0.0748\n",
      "Epoch [8/20], Step [290/563], Loss: 0.0907\n",
      "Epoch [8/20], Step [300/563], Loss: 0.0723\n",
      "Epoch [8/20], Step [310/563], Loss: 0.0713\n",
      "Epoch [8/20], Step [320/563], Loss: 0.0755\n",
      "Epoch [8/20], Step [330/563], Loss: 0.0587\n",
      "Epoch [8/20], Step [340/563], Loss: 0.0700\n",
      "Epoch [8/20], Step [350/563], Loss: 0.0736\n",
      "Epoch [8/20], Step [360/563], Loss: 0.0435\n",
      "Epoch [8/20], Step [370/563], Loss: 0.0571\n",
      "Epoch [8/20], Step [380/563], Loss: 0.0631\n",
      "Epoch [8/20], Step [390/563], Loss: 0.1067\n",
      "Epoch [8/20], Step [400/563], Loss: 0.0590\n",
      "Epoch [8/20], Step [410/563], Loss: 0.0785\n",
      "Epoch [8/20], Step [420/563], Loss: 0.0566\n",
      "Epoch [8/20], Step [430/563], Loss: 0.0493\n",
      "Epoch [8/20], Step [440/563], Loss: 0.0721\n",
      "Epoch [8/20], Step [450/563], Loss: 0.0671\n",
      "Epoch [8/20], Step [460/563], Loss: 0.0749\n",
      "Epoch [8/20], Step [470/563], Loss: 0.0548\n",
      "Epoch [8/20], Step [480/563], Loss: 0.0419\n",
      "Epoch [8/20], Step [490/563], Loss: 0.0634\n",
      "Epoch [8/20], Step [500/563], Loss: 0.0915\n",
      "Epoch [8/20], Step [510/563], Loss: 0.0516\n",
      "Epoch [8/20], Step [520/563], Loss: 0.0807\n",
      "Epoch [8/20], Step [530/563], Loss: 0.0802\n",
      "Epoch [8/20], Step [540/563], Loss: 0.0900\n",
      "Epoch [8/20], Step [550/563], Loss: 0.0896\n",
      "Epoch [8/20], Step [560/563], Loss: 0.0672\n",
      "Epoch 8, Training_Loss: 0.0682, Time: 2.26s\n",
      "Validation_Loss: 0.0662, Validation_Accuracy: 91.71%\n",
      "Epoch [9/20], Step [10/563], Loss: 0.0865\n",
      "Epoch [9/20], Step [20/563], Loss: 0.0746\n",
      "Epoch [9/20], Step [30/563], Loss: 0.0549\n",
      "Epoch [9/20], Step [40/563], Loss: 0.0611\n",
      "Epoch [9/20], Step [50/563], Loss: 0.0797\n",
      "Epoch [9/20], Step [60/563], Loss: 0.0761\n",
      "Epoch [9/20], Step [70/563], Loss: 0.0632\n",
      "Epoch [9/20], Step [80/563], Loss: 0.0643\n",
      "Epoch [9/20], Step [90/563], Loss: 0.0805\n",
      "Epoch [9/20], Step [100/563], Loss: 0.0330\n",
      "Epoch [9/20], Step [110/563], Loss: 0.0678\n",
      "Epoch [9/20], Step [120/563], Loss: 0.0846\n",
      "Epoch [9/20], Step [130/563], Loss: 0.0535\n",
      "Epoch [9/20], Step [140/563], Loss: 0.0356\n",
      "Epoch [9/20], Step [150/563], Loss: 0.0545\n",
      "Epoch [9/20], Step [160/563], Loss: 0.0906\n",
      "Epoch [9/20], Step [170/563], Loss: 0.1205\n",
      "Epoch [9/20], Step [180/563], Loss: 0.0762\n",
      "Epoch [9/20], Step [190/563], Loss: 0.0354\n",
      "Epoch [9/20], Step [200/563], Loss: 0.0725\n",
      "Epoch [9/20], Step [210/563], Loss: 0.0641\n",
      "Epoch [9/20], Step [220/563], Loss: 0.0673\n",
      "Epoch [9/20], Step [230/563], Loss: 0.0724\n",
      "Epoch [9/20], Step [240/563], Loss: 0.0948\n",
      "Epoch [9/20], Step [250/563], Loss: 0.1015\n",
      "Epoch [9/20], Step [260/563], Loss: 0.0918\n",
      "Epoch [9/20], Step [270/563], Loss: 0.0905\n",
      "Epoch [9/20], Step [280/563], Loss: 0.0777\n",
      "Epoch [9/20], Step [290/563], Loss: 0.0572\n",
      "Epoch [9/20], Step [300/563], Loss: 0.0838\n",
      "Epoch [9/20], Step [310/563], Loss: 0.0305\n",
      "Epoch [9/20], Step [320/563], Loss: 0.0562\n",
      "Epoch [9/20], Step [330/563], Loss: 0.0736\n",
      "Epoch [9/20], Step [340/563], Loss: 0.0676\n",
      "Epoch [9/20], Step [350/563], Loss: 0.0523\n",
      "Epoch [9/20], Step [360/563], Loss: 0.0929\n",
      "Epoch [9/20], Step [370/563], Loss: 0.0960\n",
      "Epoch [9/20], Step [380/563], Loss: 0.0682\n",
      "Epoch [9/20], Step [390/563], Loss: 0.0816\n",
      "Epoch [9/20], Step [400/563], Loss: 0.0423\n",
      "Epoch [9/20], Step [410/563], Loss: 0.0690\n",
      "Epoch [9/20], Step [420/563], Loss: 0.0929\n",
      "Epoch [9/20], Step [430/563], Loss: 0.0803\n",
      "Epoch [9/20], Step [440/563], Loss: 0.0545\n",
      "Epoch [9/20], Step [450/563], Loss: 0.0724\n",
      "Epoch [9/20], Step [460/563], Loss: 0.0621\n",
      "Epoch [9/20], Step [470/563], Loss: 0.0945\n",
      "Epoch [9/20], Step [480/563], Loss: 0.0464\n",
      "Epoch [9/20], Step [490/563], Loss: 0.0672\n",
      "Epoch [9/20], Step [500/563], Loss: 0.0757\n",
      "Epoch [9/20], Step [510/563], Loss: 0.0642\n",
      "Epoch [9/20], Step [520/563], Loss: 0.0520\n",
      "Epoch [9/20], Step [530/563], Loss: 0.0496\n",
      "Epoch [9/20], Step [540/563], Loss: 0.1003\n",
      "Epoch [9/20], Step [550/563], Loss: 0.0724\n",
      "Epoch [9/20], Step [560/563], Loss: 0.0512\n",
      "Epoch 9, Training_Loss: 0.0680, Time: 2.30s\n",
      "Validation_Loss: 0.0650, Validation_Accuracy: 92.09%\n",
      "Epoch [10/20], Step [10/563], Loss: 0.0539\n",
      "Epoch [10/20], Step [20/563], Loss: 0.0540\n",
      "Epoch [10/20], Step [30/563], Loss: 0.0908\n",
      "Epoch [10/20], Step [40/563], Loss: 0.0488\n",
      "Epoch [10/20], Step [50/563], Loss: 0.0946\n",
      "Epoch [10/20], Step [60/563], Loss: 0.0917\n",
      "Epoch [10/20], Step [70/563], Loss: 0.0529\n",
      "Epoch [10/20], Step [80/563], Loss: 0.0860\n",
      "Epoch [10/20], Step [90/563], Loss: 0.0767\n",
      "Epoch [10/20], Step [100/563], Loss: 0.0724\n",
      "Epoch [10/20], Step [110/563], Loss: 0.0587\n",
      "Epoch [10/20], Step [120/563], Loss: 0.1067\n",
      "Epoch [10/20], Step [130/563], Loss: 0.0490\n",
      "Epoch [10/20], Step [140/563], Loss: 0.0538\n",
      "Epoch [10/20], Step [150/563], Loss: 0.0691\n",
      "Epoch [10/20], Step [160/563], Loss: 0.0764\n",
      "Epoch [10/20], Step [170/563], Loss: 0.0711\n",
      "Epoch [10/20], Step [180/563], Loss: 0.0690\n",
      "Epoch [10/20], Step [190/563], Loss: 0.0771\n",
      "Epoch [10/20], Step [200/563], Loss: 0.1170\n",
      "Epoch [10/20], Step [210/563], Loss: 0.0730\n",
      "Epoch [10/20], Step [220/563], Loss: 0.0590\n",
      "Epoch [10/20], Step [230/563], Loss: 0.0625\n",
      "Epoch [10/20], Step [240/563], Loss: 0.0691\n",
      "Epoch [10/20], Step [250/563], Loss: 0.0873\n",
      "Epoch [10/20], Step [260/563], Loss: 0.0652\n",
      "Epoch [10/20], Step [270/563], Loss: 0.0615\n",
      "Epoch [10/20], Step [280/563], Loss: 0.0844\n",
      "Epoch [10/20], Step [290/563], Loss: 0.0773\n",
      "Epoch [10/20], Step [300/563], Loss: 0.0524\n",
      "Epoch [10/20], Step [310/563], Loss: 0.0578\n",
      "Epoch [10/20], Step [320/563], Loss: 0.0440\n",
      "Epoch [10/20], Step [330/563], Loss: 0.0706\n",
      "Epoch [10/20], Step [340/563], Loss: 0.0972\n",
      "Epoch [10/20], Step [350/563], Loss: 0.0476\n",
      "Epoch [10/20], Step [360/563], Loss: 0.0848\n",
      "Epoch [10/20], Step [370/563], Loss: 0.1082\n",
      "Epoch [10/20], Step [380/563], Loss: 0.0879\n",
      "Epoch [10/20], Step [390/563], Loss: 0.0536\n",
      "Epoch [10/20], Step [400/563], Loss: 0.0485\n",
      "Epoch [10/20], Step [410/563], Loss: 0.0739\n",
      "Epoch [10/20], Step [420/563], Loss: 0.0881\n",
      "Epoch [10/20], Step [430/563], Loss: 0.0622\n",
      "Epoch [10/20], Step [440/563], Loss: 0.0514\n",
      "Epoch [10/20], Step [450/563], Loss: 0.0588\n",
      "Epoch [10/20], Step [460/563], Loss: 0.0896\n",
      "Epoch [10/20], Step [470/563], Loss: 0.0655\n",
      "Epoch [10/20], Step [480/563], Loss: 0.0915\n",
      "Epoch [10/20], Step [490/563], Loss: 0.0683\n",
      "Epoch [10/20], Step [500/563], Loss: 0.0506\n",
      "Epoch [10/20], Step [510/563], Loss: 0.0637\n",
      "Epoch [10/20], Step [520/563], Loss: 0.0858\n",
      "Epoch [10/20], Step [530/563], Loss: 0.0925\n",
      "Epoch [10/20], Step [540/563], Loss: 0.0650\n",
      "Epoch [10/20], Step [550/563], Loss: 0.0644\n",
      "Epoch [10/20], Step [560/563], Loss: 0.0531\n",
      "Epoch 10, Training_Loss: 0.0683, Time: 2.28s\n",
      "Validation_Loss: 0.0635, Validation_Accuracy: 92.20%\n",
      "Model checkpoint saved to /home/ec2-user/SageMaker/model_epoch_10.pt\n",
      "Epoch [11/20], Step [10/563], Loss: 0.0610\n",
      "Epoch [11/20], Step [20/563], Loss: 0.0565\n",
      "Epoch [11/20], Step [30/563], Loss: 0.0717\n",
      "Epoch [11/20], Step [40/563], Loss: 0.0638\n",
      "Epoch [11/20], Step [50/563], Loss: 0.0525\n",
      "Epoch [11/20], Step [60/563], Loss: 0.0917\n",
      "Epoch [11/20], Step [70/563], Loss: 0.0789\n",
      "Epoch [11/20], Step [80/563], Loss: 0.0762\n",
      "Epoch [11/20], Step [90/563], Loss: 0.0559\n",
      "Epoch [11/20], Step [100/563], Loss: 0.0566\n",
      "Epoch [11/20], Step [110/563], Loss: 0.0350\n",
      "Epoch [11/20], Step [120/563], Loss: 0.0574\n",
      "Epoch [11/20], Step [130/563], Loss: 0.0834\n",
      "Epoch [11/20], Step [140/563], Loss: 0.0468\n",
      "Epoch [11/20], Step [150/563], Loss: 0.0473\n",
      "Epoch [11/20], Step [160/563], Loss: 0.0854\n",
      "Epoch [11/20], Step [170/563], Loss: 0.0796\n",
      "Epoch [11/20], Step [180/563], Loss: 0.0568\n",
      "Epoch [11/20], Step [190/563], Loss: 0.1001\n",
      "Epoch [11/20], Step [200/563], Loss: 0.0678\n",
      "Epoch [11/20], Step [210/563], Loss: 0.0393\n",
      "Epoch [11/20], Step [220/563], Loss: 0.0639\n",
      "Epoch [11/20], Step [230/563], Loss: 0.0802\n",
      "Epoch [11/20], Step [240/563], Loss: 0.0817\n",
      "Epoch [11/20], Step [250/563], Loss: 0.0617\n",
      "Epoch [11/20], Step [260/563], Loss: 0.0639\n",
      "Epoch [11/20], Step [270/563], Loss: 0.0858\n",
      "Epoch [11/20], Step [280/563], Loss: 0.0582\n",
      "Epoch [11/20], Step [290/563], Loss: 0.0799\n",
      "Epoch [11/20], Step [300/563], Loss: 0.0870\n",
      "Epoch [11/20], Step [310/563], Loss: 0.0781\n",
      "Epoch [11/20], Step [320/563], Loss: 0.0736\n",
      "Epoch [11/20], Step [330/563], Loss: 0.0651\n",
      "Epoch [11/20], Step [340/563], Loss: 0.0750\n",
      "Epoch [11/20], Step [350/563], Loss: 0.0705\n",
      "Epoch [11/20], Step [360/563], Loss: 0.0931\n",
      "Epoch [11/20], Step [370/563], Loss: 0.0822\n",
      "Epoch [11/20], Step [380/563], Loss: 0.0790\n",
      "Epoch [11/20], Step [390/563], Loss: 0.0523\n",
      "Epoch [11/20], Step [400/563], Loss: 0.0655\n",
      "Epoch [11/20], Step [410/563], Loss: 0.0686\n",
      "Epoch [11/20], Step [420/563], Loss: 0.0727\n",
      "Epoch [11/20], Step [430/563], Loss: 0.0917\n",
      "Epoch [11/20], Step [440/563], Loss: 0.0754\n",
      "Epoch [11/20], Step [450/563], Loss: 0.0697\n",
      "Epoch [11/20], Step [460/563], Loss: 0.0689\n",
      "Epoch [11/20], Step [470/563], Loss: 0.0652\n",
      "Epoch [11/20], Step [480/563], Loss: 0.0206\n",
      "Epoch [11/20], Step [490/563], Loss: 0.0665\n",
      "Epoch [11/20], Step [500/563], Loss: 0.0599\n",
      "Epoch [11/20], Step [510/563], Loss: 0.0143\n",
      "Epoch [11/20], Step [520/563], Loss: 0.0483\n",
      "Epoch [11/20], Step [530/563], Loss: 0.0486\n",
      "Epoch [11/20], Step [540/563], Loss: 0.0564\n",
      "Epoch [11/20], Step [550/563], Loss: 0.0806\n",
      "Epoch [11/20], Step [560/563], Loss: 0.0900\n",
      "Epoch 11, Training_Loss: 0.0673, Time: 2.31s\n",
      "Validation_Loss: 0.0662, Validation_Accuracy: 91.80%\n",
      "Epoch [12/20], Step [10/563], Loss: 0.0896\n",
      "Epoch [12/20], Step [20/563], Loss: 0.0309\n",
      "Epoch [12/20], Step [30/563], Loss: 0.0833\n",
      "Epoch [12/20], Step [40/563], Loss: 0.0540\n",
      "Epoch [12/20], Step [50/563], Loss: 0.0756\n",
      "Epoch [12/20], Step [60/563], Loss: 0.0753\n",
      "Epoch [12/20], Step [70/563], Loss: 0.0746\n",
      "Epoch [12/20], Step [80/563], Loss: 0.0426\n",
      "Epoch [12/20], Step [90/563], Loss: 0.0416\n",
      "Epoch [12/20], Step [100/563], Loss: 0.0396\n",
      "Epoch [12/20], Step [110/563], Loss: 0.0762\n",
      "Epoch [12/20], Step [120/563], Loss: 0.0495\n",
      "Epoch [12/20], Step [130/563], Loss: 0.0651\n",
      "Epoch [12/20], Step [140/563], Loss: 0.0701\n",
      "Epoch [12/20], Step [150/563], Loss: 0.0580\n",
      "Epoch [12/20], Step [160/563], Loss: 0.0578\n",
      "Epoch [12/20], Step [170/563], Loss: 0.1009\n",
      "Epoch [12/20], Step [180/563], Loss: 0.0491\n",
      "Epoch [12/20], Step [190/563], Loss: 0.1055\n",
      "Epoch [12/20], Step [200/563], Loss: 0.1025\n",
      "Epoch [12/20], Step [210/563], Loss: 0.0592\n",
      "Epoch [12/20], Step [220/563], Loss: 0.0520\n",
      "Epoch [12/20], Step [230/563], Loss: 0.0668\n",
      "Epoch [12/20], Step [240/563], Loss: 0.0850\n",
      "Epoch [12/20], Step [250/563], Loss: 0.0702\n",
      "Epoch [12/20], Step [260/563], Loss: 0.0838\n",
      "Epoch [12/20], Step [270/563], Loss: 0.0849\n",
      "Epoch [12/20], Step [280/563], Loss: 0.0625\n",
      "Epoch [12/20], Step [290/563], Loss: 0.0768\n",
      "Epoch [12/20], Step [300/563], Loss: 0.0466\n",
      "Epoch [12/20], Step [310/563], Loss: 0.0756\n",
      "Epoch [12/20], Step [320/563], Loss: 0.0738\n",
      "Epoch [12/20], Step [330/563], Loss: 0.0677\n",
      "Epoch [12/20], Step [340/563], Loss: 0.0415\n",
      "Epoch [12/20], Step [350/563], Loss: 0.0583\n",
      "Epoch [12/20], Step [360/563], Loss: 0.0624\n",
      "Epoch [12/20], Step [370/563], Loss: 0.0874\n",
      "Epoch [12/20], Step [380/563], Loss: 0.0520\n",
      "Epoch [12/20], Step [390/563], Loss: 0.0927\n",
      "Epoch [12/20], Step [400/563], Loss: 0.0803\n",
      "Epoch [12/20], Step [410/563], Loss: 0.0798\n",
      "Epoch [12/20], Step [420/563], Loss: 0.0330\n",
      "Epoch [12/20], Step [430/563], Loss: 0.1310\n",
      "Epoch [12/20], Step [440/563], Loss: 0.0839\n",
      "Epoch [12/20], Step [450/563], Loss: 0.0486\n",
      "Epoch [12/20], Step [460/563], Loss: 0.0855\n",
      "Epoch [12/20], Step [470/563], Loss: 0.0794\n",
      "Epoch [12/20], Step [480/563], Loss: 0.0562\n",
      "Epoch [12/20], Step [490/563], Loss: 0.0780\n",
      "Epoch [12/20], Step [500/563], Loss: 0.0618\n",
      "Epoch [12/20], Step [510/563], Loss: 0.0853\n",
      "Epoch [12/20], Step [520/563], Loss: 0.0457\n",
      "Epoch [12/20], Step [530/563], Loss: 0.0712\n",
      "Epoch [12/20], Step [540/563], Loss: 0.0902\n",
      "Epoch [12/20], Step [550/563], Loss: 0.0622\n",
      "Epoch [12/20], Step [560/563], Loss: 0.0717\n",
      "Epoch 12, Training_Loss: 0.0672, Time: 2.26s\n",
      "Validation_Loss: 0.0653, Validation_Accuracy: 91.94%\n",
      "Epoch [13/20], Step [10/563], Loss: 0.0942\n",
      "Epoch [13/20], Step [20/563], Loss: 0.0770\n",
      "Epoch [13/20], Step [30/563], Loss: 0.0718\n",
      "Epoch [13/20], Step [40/563], Loss: 0.0542\n",
      "Epoch [13/20], Step [50/563], Loss: 0.0360\n",
      "Epoch [13/20], Step [60/563], Loss: 0.0860\n",
      "Epoch [13/20], Step [70/563], Loss: 0.0431\n",
      "Epoch [13/20], Step [80/563], Loss: 0.0852\n",
      "Epoch [13/20], Step [90/563], Loss: 0.1305\n",
      "Epoch [13/20], Step [100/563], Loss: 0.0857\n",
      "Epoch [13/20], Step [110/563], Loss: 0.0690\n",
      "Epoch [13/20], Step [120/563], Loss: 0.0616\n",
      "Epoch [13/20], Step [130/563], Loss: 0.0303\n",
      "Epoch [13/20], Step [140/563], Loss: 0.0716\n",
      "Epoch [13/20], Step [150/563], Loss: 0.0460\n",
      "Epoch [13/20], Step [160/563], Loss: 0.0730\n",
      "Epoch [13/20], Step [170/563], Loss: 0.0682\n",
      "Epoch [13/20], Step [180/563], Loss: 0.0611\n",
      "Epoch [13/20], Step [190/563], Loss: 0.0265\n",
      "Epoch [13/20], Step [200/563], Loss: 0.0746\n",
      "Epoch [13/20], Step [210/563], Loss: 0.0603\n",
      "Epoch [13/20], Step [220/563], Loss: 0.0819\n",
      "Epoch [13/20], Step [230/563], Loss: 0.0830\n",
      "Epoch [13/20], Step [240/563], Loss: 0.0758\n",
      "Epoch [13/20], Step [250/563], Loss: 0.0536\n",
      "Epoch [13/20], Step [260/563], Loss: 0.0552\n",
      "Epoch [13/20], Step [270/563], Loss: 0.0846\n",
      "Epoch [13/20], Step [280/563], Loss: 0.0566\n",
      "Epoch [13/20], Step [290/563], Loss: 0.0569\n",
      "Epoch [13/20], Step [300/563], Loss: 0.0787\n",
      "Epoch [13/20], Step [310/563], Loss: 0.0759\n",
      "Epoch [13/20], Step [320/563], Loss: 0.0546\n",
      "Epoch [13/20], Step [330/563], Loss: 0.0623\n",
      "Epoch [13/20], Step [340/563], Loss: 0.0612\n",
      "Epoch [13/20], Step [350/563], Loss: 0.0852\n",
      "Epoch [13/20], Step [360/563], Loss: 0.0724\n",
      "Epoch [13/20], Step [370/563], Loss: 0.0557\n",
      "Epoch [13/20], Step [380/563], Loss: 0.0801\n",
      "Epoch [13/20], Step [390/563], Loss: 0.0542\n",
      "Epoch [13/20], Step [400/563], Loss: 0.0463\n",
      "Epoch [13/20], Step [410/563], Loss: 0.0578\n",
      "Epoch [13/20], Step [420/563], Loss: 0.0600\n",
      "Epoch [13/20], Step [430/563], Loss: 0.0439\n",
      "Epoch [13/20], Step [440/563], Loss: 0.0282\n",
      "Epoch [13/20], Step [450/563], Loss: 0.0718\n",
      "Epoch [13/20], Step [460/563], Loss: 0.1062\n",
      "Epoch [13/20], Step [470/563], Loss: 0.0635\n",
      "Epoch [13/20], Step [480/563], Loss: 0.0652\n",
      "Epoch [13/20], Step [490/563], Loss: 0.1002\n",
      "Epoch [13/20], Step [500/563], Loss: 0.0645\n",
      "Epoch [13/20], Step [510/563], Loss: 0.0849\n",
      "Epoch [13/20], Step [520/563], Loss: 0.0710\n",
      "Epoch [13/20], Step [530/563], Loss: 0.0743\n",
      "Epoch [13/20], Step [540/563], Loss: 0.0673\n",
      "Epoch [13/20], Step [550/563], Loss: 0.0595\n",
      "Epoch [13/20], Step [560/563], Loss: 0.0728\n",
      "Epoch 13, Training_Loss: 0.0673, Time: 2.27s\n",
      "Validation_Loss: 0.0655, Validation_Accuracy: 91.99%\n",
      "Epoch [14/20], Step [10/563], Loss: 0.0430\n",
      "Epoch [14/20], Step [20/563], Loss: 0.0544\n",
      "Epoch [14/20], Step [30/563], Loss: 0.0420\n",
      "Epoch [14/20], Step [40/563], Loss: 0.0553\n",
      "Epoch [14/20], Step [50/563], Loss: 0.0784\n",
      "Epoch [14/20], Step [60/563], Loss: 0.0863\n",
      "Epoch [14/20], Step [70/563], Loss: 0.0802\n",
      "Epoch [14/20], Step [80/563], Loss: 0.0503\n",
      "Epoch [14/20], Step [90/563], Loss: 0.1139\n",
      "Epoch [14/20], Step [100/563], Loss: 0.0652\n",
      "Epoch [14/20], Step [110/563], Loss: 0.0512\n",
      "Epoch [14/20], Step [120/563], Loss: 0.0506\n",
      "Epoch [14/20], Step [130/563], Loss: 0.0460\n",
      "Epoch [14/20], Step [140/563], Loss: 0.0503\n",
      "Epoch [14/20], Step [150/563], Loss: 0.0249\n",
      "Epoch [14/20], Step [160/563], Loss: 0.0872\n",
      "Epoch [14/20], Step [170/563], Loss: 0.0527\n",
      "Epoch [14/20], Step [180/563], Loss: 0.0489\n",
      "Epoch [14/20], Step [190/563], Loss: 0.0486\n",
      "Epoch [14/20], Step [200/563], Loss: 0.0551\n",
      "Epoch [14/20], Step [210/563], Loss: 0.0716\n",
      "Epoch [14/20], Step [220/563], Loss: 0.0535\n",
      "Epoch [14/20], Step [230/563], Loss: 0.0509\n",
      "Epoch [14/20], Step [240/563], Loss: 0.1091\n",
      "Epoch [14/20], Step [250/563], Loss: 0.0813\n",
      "Epoch [14/20], Step [260/563], Loss: 0.1076\n",
      "Epoch [14/20], Step [270/563], Loss: 0.0492\n",
      "Epoch [14/20], Step [280/563], Loss: 0.0569\n",
      "Epoch [14/20], Step [290/563], Loss: 0.0851\n",
      "Epoch [14/20], Step [300/563], Loss: 0.1025\n",
      "Epoch [14/20], Step [310/563], Loss: 0.0512\n",
      "Epoch [14/20], Step [320/563], Loss: 0.0353\n",
      "Epoch [14/20], Step [330/563], Loss: 0.0470\n",
      "Epoch [14/20], Step [340/563], Loss: 0.0479\n",
      "Epoch [14/20], Step [350/563], Loss: 0.0589\n",
      "Epoch [14/20], Step [360/563], Loss: 0.0699\n",
      "Epoch [14/20], Step [370/563], Loss: 0.0518\n",
      "Epoch [14/20], Step [380/563], Loss: 0.0922\n",
      "Epoch [14/20], Step [390/563], Loss: 0.0349\n",
      "Epoch [14/20], Step [400/563], Loss: 0.0533\n",
      "Epoch [14/20], Step [410/563], Loss: 0.0467\n",
      "Epoch [14/20], Step [420/563], Loss: 0.0774\n",
      "Epoch [14/20], Step [430/563], Loss: 0.0513\n",
      "Epoch [14/20], Step [440/563], Loss: 0.1144\n",
      "Epoch [14/20], Step [450/563], Loss: 0.0401\n",
      "Epoch [14/20], Step [460/563], Loss: 0.0541\n",
      "Epoch [14/20], Step [470/563], Loss: 0.0607\n",
      "Epoch [14/20], Step [480/563], Loss: 0.0949\n",
      "Epoch [14/20], Step [490/563], Loss: 0.0716\n",
      "Epoch [14/20], Step [500/563], Loss: 0.0686\n",
      "Epoch [14/20], Step [510/563], Loss: 0.0389\n",
      "Epoch [14/20], Step [520/563], Loss: 0.0645\n",
      "Epoch [14/20], Step [530/563], Loss: 0.0667\n",
      "Epoch [14/20], Step [540/563], Loss: 0.0407\n",
      "Epoch [14/20], Step [550/563], Loss: 0.0818\n",
      "Epoch [14/20], Step [560/563], Loss: 0.0855\n",
      "Epoch 14, Training_Loss: 0.0662, Time: 2.34s\n",
      "Validation_Loss: 0.0627, Validation_Accuracy: 92.14%\n",
      "Epoch [15/20], Step [10/563], Loss: 0.0596\n",
      "Epoch [15/20], Step [20/563], Loss: 0.0492\n",
      "Epoch [15/20], Step [30/563], Loss: 0.0933\n",
      "Epoch [15/20], Step [40/563], Loss: 0.0514\n",
      "Epoch [15/20], Step [50/563], Loss: 0.0752\n",
      "Epoch [15/20], Step [60/563], Loss: 0.0914\n",
      "Epoch [15/20], Step [70/563], Loss: 0.0811\n",
      "Epoch [15/20], Step [80/563], Loss: 0.0453\n",
      "Epoch [15/20], Step [90/563], Loss: 0.0747\n",
      "Epoch [15/20], Step [100/563], Loss: 0.0893\n",
      "Epoch [15/20], Step [110/563], Loss: 0.0860\n",
      "Epoch [15/20], Step [120/563], Loss: 0.0711\n",
      "Epoch [15/20], Step [130/563], Loss: 0.0542\n",
      "Epoch [15/20], Step [140/563], Loss: 0.0623\n",
      "Epoch [15/20], Step [150/563], Loss: 0.0604\n",
      "Epoch [15/20], Step [160/563], Loss: 0.0645\n",
      "Epoch [15/20], Step [170/563], Loss: 0.0843\n",
      "Epoch [15/20], Step [180/563], Loss: 0.0805\n",
      "Epoch [15/20], Step [190/563], Loss: 0.0620\n",
      "Epoch [15/20], Step [200/563], Loss: 0.0558\n",
      "Epoch [15/20], Step [210/563], Loss: 0.0846\n",
      "Epoch [15/20], Step [220/563], Loss: 0.0412\n",
      "Epoch [15/20], Step [230/563], Loss: 0.0527\n",
      "Epoch [15/20], Step [240/563], Loss: 0.0669\n",
      "Epoch [15/20], Step [250/563], Loss: 0.0435\n",
      "Epoch [15/20], Step [260/563], Loss: 0.0458\n",
      "Epoch [15/20], Step [270/563], Loss: 0.0555\n",
      "Epoch [15/20], Step [280/563], Loss: 0.0791\n",
      "Epoch [15/20], Step [290/563], Loss: 0.1074\n",
      "Epoch [15/20], Step [300/563], Loss: 0.0719\n",
      "Epoch [15/20], Step [310/563], Loss: 0.1037\n",
      "Epoch [15/20], Step [320/563], Loss: 0.0857\n",
      "Epoch [15/20], Step [330/563], Loss: 0.0651\n",
      "Epoch [15/20], Step [340/563], Loss: 0.0549\n",
      "Epoch [15/20], Step [350/563], Loss: 0.0538\n",
      "Epoch [15/20], Step [360/563], Loss: 0.0665\n",
      "Epoch [15/20], Step [370/563], Loss: 0.0749\n",
      "Epoch [15/20], Step [380/563], Loss: 0.0384\n",
      "Epoch [15/20], Step [390/563], Loss: 0.0415\n",
      "Epoch [15/20], Step [400/563], Loss: 0.0712\n",
      "Epoch [15/20], Step [410/563], Loss: 0.0605\n",
      "Epoch [15/20], Step [420/563], Loss: 0.0573\n",
      "Epoch [15/20], Step [430/563], Loss: 0.0679\n",
      "Epoch [15/20], Step [440/563], Loss: 0.0587\n",
      "Epoch [15/20], Step [450/563], Loss: 0.0614\n",
      "Epoch [15/20], Step [460/563], Loss: 0.0916\n",
      "Epoch [15/20], Step [470/563], Loss: 0.0500\n",
      "Epoch [15/20], Step [480/563], Loss: 0.0711\n",
      "Epoch [15/20], Step [490/563], Loss: 0.0618\n",
      "Epoch [15/20], Step [500/563], Loss: 0.0654\n",
      "Epoch [15/20], Step [510/563], Loss: 0.0886\n",
      "Epoch [15/20], Step [520/563], Loss: 0.0784\n",
      "Epoch [15/20], Step [530/563], Loss: 0.0740\n",
      "Epoch [15/20], Step [540/563], Loss: 0.0690\n",
      "Epoch [15/20], Step [550/563], Loss: 0.0509\n",
      "Epoch [15/20], Step [560/563], Loss: 0.0512\n",
      "Epoch 15, Training_Loss: 0.0653, Time: 2.25s\n",
      "Validation_Loss: 0.0644, Validation_Accuracy: 91.91%\n",
      "Model checkpoint saved to /home/ec2-user/SageMaker/model_epoch_15.pt\n",
      "Epoch [16/20], Step [10/563], Loss: 0.0768\n",
      "Epoch [16/20], Step [20/563], Loss: 0.0770\n",
      "Epoch [16/20], Step [30/563], Loss: 0.0743\n",
      "Epoch [16/20], Step [40/563], Loss: 0.0616\n",
      "Epoch [16/20], Step [50/563], Loss: 0.0506\n",
      "Epoch [16/20], Step [60/563], Loss: 0.0811\n",
      "Epoch [16/20], Step [70/563], Loss: 0.0618\n",
      "Epoch [16/20], Step [80/563], Loss: 0.0558\n",
      "Epoch [16/20], Step [90/563], Loss: 0.0751\n",
      "Epoch [16/20], Step [100/563], Loss: 0.0528\n",
      "Epoch [16/20], Step [110/563], Loss: 0.0872\n",
      "Epoch [16/20], Step [120/563], Loss: 0.0362\n",
      "Epoch [16/20], Step [130/563], Loss: 0.0515\n",
      "Epoch [16/20], Step [140/563], Loss: 0.0683\n",
      "Epoch [16/20], Step [150/563], Loss: 0.1236\n",
      "Epoch [16/20], Step [160/563], Loss: 0.0462\n",
      "Epoch [16/20], Step [170/563], Loss: 0.0739\n",
      "Epoch [16/20], Step [180/563], Loss: 0.0431\n",
      "Epoch [16/20], Step [190/563], Loss: 0.0495\n",
      "Epoch [16/20], Step [200/563], Loss: 0.0794\n",
      "Epoch [16/20], Step [210/563], Loss: 0.0697\n",
      "Epoch [16/20], Step [220/563], Loss: 0.0636\n",
      "Epoch [16/20], Step [230/563], Loss: 0.0417\n",
      "Epoch [16/20], Step [240/563], Loss: 0.0473\n",
      "Epoch [16/20], Step [250/563], Loss: 0.0777\n",
      "Epoch [16/20], Step [260/563], Loss: 0.0592\n",
      "Epoch [16/20], Step [270/563], Loss: 0.0352\n",
      "Epoch [16/20], Step [280/563], Loss: 0.0515\n",
      "Epoch [16/20], Step [290/563], Loss: 0.0318\n",
      "Epoch [16/20], Step [300/563], Loss: 0.0502\n",
      "Epoch [16/20], Step [310/563], Loss: 0.0817\n",
      "Epoch [16/20], Step [320/563], Loss: 0.0260\n",
      "Epoch [16/20], Step [330/563], Loss: 0.0855\n",
      "Epoch [16/20], Step [340/563], Loss: 0.0766\n",
      "Epoch [16/20], Step [350/563], Loss: 0.0755\n",
      "Epoch [16/20], Step [360/563], Loss: 0.0892\n",
      "Epoch [16/20], Step [370/563], Loss: 0.0394\n",
      "Epoch [16/20], Step [380/563], Loss: 0.0506\n",
      "Epoch [16/20], Step [390/563], Loss: 0.0416\n",
      "Epoch [16/20], Step [400/563], Loss: 0.0830\n",
      "Epoch [16/20], Step [410/563], Loss: 0.0596\n",
      "Epoch [16/20], Step [420/563], Loss: 0.0954\n",
      "Epoch [16/20], Step [430/563], Loss: 0.0455\n",
      "Epoch [16/20], Step [440/563], Loss: 0.0561\n",
      "Epoch [16/20], Step [450/563], Loss: 0.0670\n",
      "Epoch [16/20], Step [460/563], Loss: 0.0413\n",
      "Epoch [16/20], Step [470/563], Loss: 0.0882\n",
      "Epoch [16/20], Step [480/563], Loss: 0.0842\n",
      "Epoch [16/20], Step [490/563], Loss: 0.0624\n",
      "Epoch [16/20], Step [500/563], Loss: 0.0350\n",
      "Epoch [16/20], Step [510/563], Loss: 0.0681\n",
      "Epoch [16/20], Step [520/563], Loss: 0.0556\n",
      "Epoch [16/20], Step [530/563], Loss: 0.0594\n",
      "Epoch [16/20], Step [540/563], Loss: 0.0763\n",
      "Epoch [16/20], Step [550/563], Loss: 0.0527\n",
      "Epoch [16/20], Step [560/563], Loss: 0.0608\n",
      "Epoch 16, Training_Loss: 0.0658, Time: 2.33s\n",
      "Validation_Loss: 0.0646, Validation_Accuracy: 91.91%\n",
      "Epoch [17/20], Step [10/563], Loss: 0.0775\n",
      "Epoch [17/20], Step [20/563], Loss: 0.0699\n",
      "Epoch [17/20], Step [30/563], Loss: 0.0776\n",
      "Epoch [17/20], Step [40/563], Loss: 0.0627\n",
      "Epoch [17/20], Step [50/563], Loss: 0.0892\n",
      "Epoch [17/20], Step [60/563], Loss: 0.0586\n",
      "Epoch [17/20], Step [70/563], Loss: 0.0572\n",
      "Epoch [17/20], Step [80/563], Loss: 0.0980\n",
      "Epoch [17/20], Step [90/563], Loss: 0.0649\n",
      "Epoch [17/20], Step [100/563], Loss: 0.0767\n",
      "Epoch [17/20], Step [110/563], Loss: 0.0599\n",
      "Epoch [17/20], Step [120/563], Loss: 0.0781\n",
      "Epoch [17/20], Step [130/563], Loss: 0.0729\n",
      "Epoch [17/20], Step [140/563], Loss: 0.0443\n",
      "Epoch [17/20], Step [150/563], Loss: 0.1057\n",
      "Epoch [17/20], Step [160/563], Loss: 0.0565\n",
      "Epoch [17/20], Step [170/563], Loss: 0.0771\n",
      "Epoch [17/20], Step [180/563], Loss: 0.0883\n",
      "Epoch [17/20], Step [190/563], Loss: 0.0358\n",
      "Epoch [17/20], Step [200/563], Loss: 0.0567\n",
      "Epoch [17/20], Step [210/563], Loss: 0.0550\n",
      "Epoch [17/20], Step [220/563], Loss: 0.0520\n",
      "Epoch [17/20], Step [230/563], Loss: 0.0334\n",
      "Epoch [17/20], Step [240/563], Loss: 0.0818\n",
      "Epoch [17/20], Step [250/563], Loss: 0.0227\n",
      "Epoch [17/20], Step [260/563], Loss: 0.0807\n",
      "Epoch [17/20], Step [270/563], Loss: 0.0543\n",
      "Epoch [17/20], Step [280/563], Loss: 0.0380\n",
      "Epoch [17/20], Step [290/563], Loss: 0.0606\n",
      "Epoch [17/20], Step [300/563], Loss: 0.0467\n",
      "Epoch [17/20], Step [310/563], Loss: 0.0588\n",
      "Epoch [17/20], Step [320/563], Loss: 0.0760\n",
      "Epoch [17/20], Step [330/563], Loss: 0.0799\n",
      "Epoch [17/20], Step [340/563], Loss: 0.0594\n",
      "Epoch [17/20], Step [350/563], Loss: 0.0753\n",
      "Epoch [17/20], Step [360/563], Loss: 0.0461\n",
      "Epoch [17/20], Step [370/563], Loss: 0.0749\n",
      "Epoch [17/20], Step [380/563], Loss: 0.0897\n",
      "Epoch [17/20], Step [390/563], Loss: 0.0705\n",
      "Epoch [17/20], Step [400/563], Loss: 0.0596\n",
      "Epoch [17/20], Step [410/563], Loss: 0.0896\n",
      "Epoch [17/20], Step [420/563], Loss: 0.0741\n",
      "Epoch [17/20], Step [430/563], Loss: 0.1319\n",
      "Epoch [17/20], Step [440/563], Loss: 0.0673\n",
      "Epoch [17/20], Step [450/563], Loss: 0.0866\n",
      "Epoch [17/20], Step [460/563], Loss: 0.0643\n",
      "Epoch [17/20], Step [470/563], Loss: 0.0561\n",
      "Epoch [17/20], Step [480/563], Loss: 0.0721\n",
      "Epoch [17/20], Step [490/563], Loss: 0.0616\n",
      "Epoch [17/20], Step [500/563], Loss: 0.0557\n",
      "Epoch [17/20], Step [510/563], Loss: 0.0459\n",
      "Epoch [17/20], Step [520/563], Loss: 0.0639\n",
      "Epoch [17/20], Step [530/563], Loss: 0.0894\n",
      "Epoch [17/20], Step [540/563], Loss: 0.0798\n",
      "Epoch [17/20], Step [550/563], Loss: 0.0727\n",
      "Epoch [17/20], Step [560/563], Loss: 0.0771\n",
      "Epoch 17, Training_Loss: 0.0653, Time: 2.28s\n",
      "Validation_Loss: 0.0632, Validation_Accuracy: 92.13%\n",
      "Epoch [18/20], Step [10/563], Loss: 0.0481\n",
      "Epoch [18/20], Step [20/563], Loss: 0.0539\n",
      "Epoch [18/20], Step [30/563], Loss: 0.0767\n",
      "Epoch [18/20], Step [40/563], Loss: 0.0439\n",
      "Epoch [18/20], Step [50/563], Loss: 0.0650\n",
      "Epoch [18/20], Step [60/563], Loss: 0.0404\n",
      "Epoch [18/20], Step [70/563], Loss: 0.0502\n",
      "Epoch [18/20], Step [80/563], Loss: 0.0712\n",
      "Epoch [18/20], Step [90/563], Loss: 0.0521\n",
      "Epoch [18/20], Step [100/563], Loss: 0.0626\n",
      "Epoch [18/20], Step [110/563], Loss: 0.0362\n",
      "Epoch [18/20], Step [120/563], Loss: 0.0298\n",
      "Epoch [18/20], Step [130/563], Loss: 0.0753\n",
      "Epoch [18/20], Step [140/563], Loss: 0.0651\n",
      "Epoch [18/20], Step [150/563], Loss: 0.0498\n",
      "Epoch [18/20], Step [160/563], Loss: 0.0422\n",
      "Epoch [18/20], Step [170/563], Loss: 0.0647\n",
      "Epoch [18/20], Step [180/563], Loss: 0.0827\n",
      "Epoch [18/20], Step [190/563], Loss: 0.0519\n",
      "Epoch [18/20], Step [200/563], Loss: 0.0548\n",
      "Epoch [18/20], Step [210/563], Loss: 0.0480\n",
      "Epoch [18/20], Step [220/563], Loss: 0.0581\n",
      "Epoch [18/20], Step [230/563], Loss: 0.0527\n",
      "Epoch [18/20], Step [240/563], Loss: 0.0594\n",
      "Epoch [18/20], Step [250/563], Loss: 0.0937\n",
      "Epoch [18/20], Step [260/563], Loss: 0.0643\n",
      "Epoch [18/20], Step [270/563], Loss: 0.0511\n",
      "Epoch [18/20], Step [280/563], Loss: 0.0569\n",
      "Epoch [18/20], Step [290/563], Loss: 0.0660\n",
      "Epoch [18/20], Step [300/563], Loss: 0.0673\n",
      "Epoch [18/20], Step [310/563], Loss: 0.0586\n",
      "Epoch [18/20], Step [320/563], Loss: 0.1000\n",
      "Epoch [18/20], Step [330/563], Loss: 0.0428\n",
      "Epoch [18/20], Step [340/563], Loss: 0.0557\n",
      "Epoch [18/20], Step [350/563], Loss: 0.1238\n",
      "Epoch [18/20], Step [360/563], Loss: 0.0575\n",
      "Epoch [18/20], Step [370/563], Loss: 0.0593\n",
      "Epoch [18/20], Step [380/563], Loss: 0.0564\n",
      "Epoch [18/20], Step [390/563], Loss: 0.0558\n",
      "Epoch [18/20], Step [400/563], Loss: 0.0845\n",
      "Epoch [18/20], Step [410/563], Loss: 0.0976\n",
      "Epoch [18/20], Step [420/563], Loss: 0.0790\n",
      "Epoch [18/20], Step [430/563], Loss: 0.0564\n",
      "Epoch [18/20], Step [440/563], Loss: 0.0860\n",
      "Epoch [18/20], Step [450/563], Loss: 0.0710\n",
      "Epoch [18/20], Step [460/563], Loss: 0.0391\n",
      "Epoch [18/20], Step [470/563], Loss: 0.0803\n",
      "Epoch [18/20], Step [480/563], Loss: 0.0349\n",
      "Epoch [18/20], Step [490/563], Loss: 0.0883\n",
      "Epoch [18/20], Step [500/563], Loss: 0.0635\n",
      "Epoch [18/20], Step [510/563], Loss: 0.0860\n",
      "Epoch [18/20], Step [520/563], Loss: 0.0547\n",
      "Epoch [18/20], Step [530/563], Loss: 0.0579\n",
      "Epoch [18/20], Step [540/563], Loss: 0.1070\n",
      "Epoch [18/20], Step [550/563], Loss: 0.0885\n",
      "Epoch [18/20], Step [560/563], Loss: 0.0601\n",
      "Epoch 18, Training_Loss: 0.0649, Time: 2.25s\n",
      "Validation_Loss: 0.0623, Validation_Accuracy: 92.33%\n",
      "Epoch [19/20], Step [10/563], Loss: 0.0730\n",
      "Epoch [19/20], Step [20/563], Loss: 0.0432\n",
      "Epoch [19/20], Step [30/563], Loss: 0.0648\n",
      "Epoch [19/20], Step [40/563], Loss: 0.0684\n",
      "Epoch [19/20], Step [50/563], Loss: 0.0845\n",
      "Epoch [19/20], Step [60/563], Loss: 0.0992\n",
      "Epoch [19/20], Step [70/563], Loss: 0.0728\n",
      "Epoch [19/20], Step [80/563], Loss: 0.1026\n",
      "Epoch [19/20], Step [90/563], Loss: 0.0842\n",
      "Epoch [19/20], Step [100/563], Loss: 0.0576\n",
      "Epoch [19/20], Step [110/563], Loss: 0.0576\n",
      "Epoch [19/20], Step [120/563], Loss: 0.0816\n",
      "Epoch [19/20], Step [130/563], Loss: 0.0350\n",
      "Epoch [19/20], Step [140/563], Loss: 0.0505\n",
      "Epoch [19/20], Step [150/563], Loss: 0.0966\n",
      "Epoch [19/20], Step [160/563], Loss: 0.0567\n",
      "Epoch [19/20], Step [170/563], Loss: 0.0736\n",
      "Epoch [19/20], Step [180/563], Loss: 0.0770\n",
      "Epoch [19/20], Step [190/563], Loss: 0.0801\n",
      "Epoch [19/20], Step [200/563], Loss: 0.0643\n",
      "Epoch [19/20], Step [210/563], Loss: 0.0563\n",
      "Epoch [19/20], Step [220/563], Loss: 0.0868\n",
      "Epoch [19/20], Step [230/563], Loss: 0.0724\n",
      "Epoch [19/20], Step [240/563], Loss: 0.0800\n",
      "Epoch [19/20], Step [250/563], Loss: 0.0929\n",
      "Epoch [19/20], Step [260/563], Loss: 0.0482\n",
      "Epoch [19/20], Step [270/563], Loss: 0.0628\n",
      "Epoch [19/20], Step [280/563], Loss: 0.0510\n",
      "Epoch [19/20], Step [290/563], Loss: 0.1039\n",
      "Epoch [19/20], Step [300/563], Loss: 0.0509\n",
      "Epoch [19/20], Step [310/563], Loss: 0.0481\n",
      "Epoch [19/20], Step [320/563], Loss: 0.0742\n",
      "Epoch [19/20], Step [330/563], Loss: 0.0799\n",
      "Epoch [19/20], Step [340/563], Loss: 0.0614\n",
      "Epoch [19/20], Step [350/563], Loss: 0.0320\n",
      "Epoch [19/20], Step [360/563], Loss: 0.0794\n",
      "Epoch [19/20], Step [370/563], Loss: 0.0888\n",
      "Epoch [19/20], Step [380/563], Loss: 0.0328\n",
      "Epoch [19/20], Step [390/563], Loss: 0.0834\n",
      "Epoch [19/20], Step [400/563], Loss: 0.0646\n",
      "Epoch [19/20], Step [410/563], Loss: 0.0850\n",
      "Epoch [19/20], Step [420/563], Loss: 0.0442\n",
      "Epoch [19/20], Step [430/563], Loss: 0.0693\n",
      "Epoch [19/20], Step [440/563], Loss: 0.0855\n",
      "Epoch [19/20], Step [450/563], Loss: 0.0406\n",
      "Epoch [19/20], Step [460/563], Loss: 0.0547\n",
      "Epoch [19/20], Step [470/563], Loss: 0.0523\n",
      "Epoch [19/20], Step [480/563], Loss: 0.0738\n",
      "Epoch [19/20], Step [490/563], Loss: 0.0555\n",
      "Epoch [19/20], Step [500/563], Loss: 0.0593\n",
      "Epoch [19/20], Step [510/563], Loss: 0.0514\n",
      "Epoch [19/20], Step [520/563], Loss: 0.0486\n",
      "Epoch [19/20], Step [530/563], Loss: 0.0811\n",
      "Epoch [19/20], Step [540/563], Loss: 0.0783\n",
      "Epoch [19/20], Step [550/563], Loss: 0.0290\n",
      "Epoch [19/20], Step [560/563], Loss: 0.0672\n",
      "Epoch 19, Training_Loss: 0.0644, Time: 2.23s\n",
      "Validation_Loss: 0.0639, Validation_Accuracy: 92.03%\n",
      "Epoch [20/20], Step [10/563], Loss: 0.0455\n",
      "Epoch [20/20], Step [20/563], Loss: 0.0498\n",
      "Epoch [20/20], Step [30/563], Loss: 0.1013\n",
      "Epoch [20/20], Step [40/563], Loss: 0.1053\n",
      "Epoch [20/20], Step [50/563], Loss: 0.0417\n",
      "Epoch [20/20], Step [60/563], Loss: 0.0665\n",
      "Epoch [20/20], Step [70/563], Loss: 0.0706\n",
      "Epoch [20/20], Step [80/563], Loss: 0.0630\n",
      "Epoch [20/20], Step [90/563], Loss: 0.0652\n",
      "Epoch [20/20], Step [100/563], Loss: 0.0448\n",
      "Epoch [20/20], Step [110/563], Loss: 0.0811\n",
      "Epoch [20/20], Step [120/563], Loss: 0.0795\n",
      "Epoch [20/20], Step [130/563], Loss: 0.0886\n",
      "Epoch [20/20], Step [140/563], Loss: 0.0762\n",
      "Epoch [20/20], Step [150/563], Loss: 0.0888\n",
      "Epoch [20/20], Step [160/563], Loss: 0.0714\n",
      "Epoch [20/20], Step [170/563], Loss: 0.0613\n",
      "Epoch [20/20], Step [180/563], Loss: 0.1164\n",
      "Epoch [20/20], Step [190/563], Loss: 0.0847\n",
      "Epoch [20/20], Step [200/563], Loss: 0.0751\n",
      "Epoch [20/20], Step [210/563], Loss: 0.0465\n",
      "Epoch [20/20], Step [220/563], Loss: 0.0884\n",
      "Epoch [20/20], Step [230/563], Loss: 0.0748\n",
      "Epoch [20/20], Step [240/563], Loss: 0.0613\n",
      "Epoch [20/20], Step [250/563], Loss: 0.0655\n",
      "Epoch [20/20], Step [260/563], Loss: 0.0444\n",
      "Epoch [20/20], Step [270/563], Loss: 0.0572\n",
      "Epoch [20/20], Step [280/563], Loss: 0.0821\n",
      "Epoch [20/20], Step [290/563], Loss: 0.0830\n",
      "Epoch [20/20], Step [300/563], Loss: 0.0483\n",
      "Epoch [20/20], Step [310/563], Loss: 0.0567\n",
      "Epoch [20/20], Step [320/563], Loss: 0.0682\n",
      "Epoch [20/20], Step [330/563], Loss: 0.0703\n",
      "Epoch [20/20], Step [340/563], Loss: 0.0532\n",
      "Epoch [20/20], Step [350/563], Loss: 0.0601\n",
      "Epoch [20/20], Step [360/563], Loss: 0.0634\n",
      "Epoch [20/20], Step [370/563], Loss: 0.0804\n",
      "Epoch [20/20], Step [380/563], Loss: 0.0795\n",
      "Epoch [20/20], Step [390/563], Loss: 0.0856\n",
      "Epoch [20/20], Step [400/563], Loss: 0.0473\n",
      "Epoch [20/20], Step [410/563], Loss: 0.0506\n",
      "Epoch [20/20], Step [420/563], Loss: 0.1183\n",
      "Epoch [20/20], Step [430/563], Loss: 0.0697\n",
      "Epoch [20/20], Step [440/563], Loss: 0.0553\n",
      "Epoch [20/20], Step [450/563], Loss: 0.0578\n",
      "Epoch [20/20], Step [460/563], Loss: 0.0513\n",
      "Epoch [20/20], Step [470/563], Loss: 0.0535\n",
      "Epoch [20/20], Step [480/563], Loss: 0.0477\n",
      "Epoch [20/20], Step [490/563], Loss: 0.1049\n",
      "Epoch [20/20], Step [500/563], Loss: 0.0550\n",
      "Epoch [20/20], Step [510/563], Loss: 0.0762\n",
      "Epoch [20/20], Step [520/563], Loss: 0.0734\n",
      "Epoch [20/20], Step [530/563], Loss: 0.0673\n",
      "Epoch [20/20], Step [540/563], Loss: 0.0362\n",
      "Epoch [20/20], Step [550/563], Loss: 0.0922\n",
      "Epoch [20/20], Step [560/563], Loss: 0.0623\n",
      "Epoch 20, Training_Loss: 0.0645, Time: 2.23s\n",
      "Validation_Loss: 0.0615, Validation_Accuracy: 92.43%\n",
      "Model checkpoint saved to /home/ec2-user/SageMaker/model_epoch_20.pt\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "import time\n",
    "import os\n",
    "num_epochs = 20\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    start_time =time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        device = nn.device('cuda' if nn.cuda.is_available() else 'cpu')\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer = nn.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Ensure labels have the same shape as outputs [batch_size, 1]\n",
    "        if len(labels.shape) == 3:  # If shape is [batch_size, 1, 1]\n",
    "            labels = labels.squeeze(2)  # Convert to [batch_size, 1]\n",
    "        elif len(labels.shape) == 1:  # If shape is [batch_size]\n",
    "            labels = labels.unsqueeze(1)  # Convert to [batch_size, 1]\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f'Epoch {epoch+1}, Training_Loss: {running_loss/len(train_loader):.4f}, Time: {epoch_time:.2f}s')\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure labels have the same shape as outputs\n",
    "            if len(labels.shape) == 3:  # If shape is [batch_size, 1, 1]\n",
    "                labels = labels.squeeze(2)  # Convert to [batch_size, 1]\n",
    "            elif len(labels.shape) == 1:  # If shape is [batch_size]\n",
    "                labels = labels.unsqueeze(1)  # Convert to [batch_size, 1]\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation_Loss: {val_loss/len(val_loader):.4f}, Validation_Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # Save checkpoint every 5 epochs or at the end\n",
    "    save_dir = '/home/ec2-user/SageMaker'\n",
    "    if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n",
    "        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': val_accuracy,\n",
    "            'scaler': mms,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Model checkpoint saved to {checkpoint_path}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9df7469e-eff0-4b6e-9377-0b85d031903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Test Accuracy: 0.9100\n",
      "Precision: 0.8994\n",
      "Recall: 0.9233\n",
      "F1 Score: 0.9112\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved to /home/ec2-user/SageMaker/malconv_final.pt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Transform the test data\n",
    "X_test = mms.transform(X_test)\n",
    "X_test = np.reshape(X_test, (-1, 1, X_test.shape[1]))\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create a test dataset and dataloader\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store model predictions and actual labels\n",
    "predictions = []\n",
    "actual_labels = []\n",
    "\n",
    "print(\"Evaluating model on test set...\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Ensure labels have the same shape as outputs\n",
    "        if len(labels.shape) == 3:  # If shape is [batch_size, 1, 1]\n",
    "            labels = labels.squeeze(2)  # Convert to [batch_size, 1]\n",
    "        elif len(labels.shape) == 1:  # If shape is [batch_size]\n",
    "            labels = labels.unsqueeze(1)  # Convert to [batch_size, 1]\n",
    "        \n",
    "        # Convert probabilities to binary predictions\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        actual_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to flat arrays\n",
    "predictions = np.array(predictions).flatten()\n",
    "actual_labels = np.array(actual_labels).flatten()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(actual_labels, predictions)\n",
    "precision = precision_score(actual_labels, predictions)\n",
    "recall = recall_score(actual_labels, predictions)\n",
    "f1 = f1_score(actual_labels, predictions)\n",
    "conf_matrix = confusion_matrix(actual_labels, predictions)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Benign', 'Malware'],\n",
    "           yticklabels=['Benign', 'Malware'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save the final model for deployment\n",
    "final_model_path = os.path.join(save_dir, 'malconv_final.pt')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'scaler': mms,\n",
    "    'input_size': X_train.shape[2],\n",
    "    'hidden_size': 128,\n",
    "    'output_dim': 1,\n",
    "}, final_model_path)\n",
    "\n",
    "print(f'Final model saved to {final_model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3fb03-5f52-4792-95cd-83e00dd9ef79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6b804ee-230b-4d9a-ba9e-f4f5825984ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"deployment/model_final_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b421302-ab57-46d8-9505-e4d28bfc64cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tarball 'deployment/model_final_2.tar.gz' created successfully!\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "tarball_name = \"deployment/model_final_2.tar.gz\"\n",
    "with tarfile.open(tarball_name, \"w:gz\") as tar:\n",
    "    tar.add(\"deployment/model_final_2.pt\", arcname=\"model_final_2.pt\")\n",
    "    tar.add(\"deployment/serve.py\", arcname=\"serve.py\")\n",
    "    tar.add(\"scaler.joblib\", arcname=\"scaler.joblib\")\n",
    "\n",
    "print(f\"Tarball '{tarball_name}' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4514ce6-5a3a-484f-ac1a-72021751ffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- ec2-user/ec2-user 1288836 2025-03-24 14:18 model_final_2.pt\n",
      "-rw-rw-r-- ec2-user/ec2-user    2337 2025-03-23 04:47 serve.py\n",
      "-rw-rw-r-- ec2-user/ec2-user   57743 2025-03-24 14:11 scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "!tar -tvf deployment/model_final_2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f71503-9b45-4df1-82ea-6caf4f48566f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#s3_Bucket\n",
    "s3_bucket = \"sagemaker-us-east-1-548693114540\"\n",
    "s3_prefix = \"Malware-detection-v1\"\n",
    "model_file = \"model_.tar.gz\"  # Your local model file\n",
    "\n",
    "# Upload the model to S3\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(model_file, s3_bucket, f\"{s3_prefix}/model.tar.gz\")\n",
    "\n",
    "print(f\"Model uploaded to s3://{s3_bucket}/{s3_prefix}/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c43f8-6d64-42c6-a29e-27539bac715c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d933ff-6f0b-46a3-94a9-1d8f3f536ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0eefa1-cec7-44a9-b43f-d34097a423b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aab402-9650-4d98-a33c-f888e8a18f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cb6fa9-c648-4c2a-80d0-d672a7b42849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886dbb2-84c5-4e1e-a2b1-9260800971ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24099d3-d2e2-40a8-bfdd-6f95c8537aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
